{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AGV4FM3znB2"
      },
      "source": [
        "# Introduction to GROVER\n",
        "\n",
        "In this tutorial, we will go over what Grover is, and how to get it up and running.\n",
        "\n",
        "GROVER, or, Graph Representation frOm selfsuperVised mEssage passing tRansformer, is a novel framework proposed by Tencent AI Lab. GROVER utilizes self-supervised tasks in the node, edge and graph level in order to learn rich structural and semantic information of molecules from large unlabelled molecular datasets. GROVER integrates Message Passing Networks into a Transformer-style architecture to deliver more expressive molecular encoding.\n",
        "\n",
        "Reference Paper: [Rong, Yu, et al. \"Grover: Self-supervised message passing transformer on large-scale molecular data.\" Advances in Neural Information Processing Systems (2020).](https://drug.ai.tencent.com/publications/GROVER.pdf)\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Introduction_to_GROVER.ipynb)\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run DeepChem within Colab, you'll need to run the following installation commands. This will take about 5 minutes to run to completion and install your environment. You can of course run this tutorial locally if you prefer. In that case, don't run these cells since they will download and install Anaconda on your local machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gySfXfHP15A2"
      },
      "source": [
        "## Import and Setup required modules.\n",
        "We will first clone the repository onto the preferred platform, then install it as a library. We will also import deepchem and install descriptastorus.\n",
        "\n",
        "NOTE: The [original GROVER repository](https://github.com/tencent-ailab/grover) does not contain a `setup.py` file, thus we are currently using a fork which does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc9dofL-kbyf",
        "outputId": "7c8001e2-a269-400c-9c18-0763754c37c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n",
            "fatal: destination path 'grover' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the forked repository.\n",
        "%cd drive/MyDrive\n",
        "!git clone https://github.com/atreyamaj/grover.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsAZo_sz5nRv",
        "outputId": "75038385-b3cd-42b8-943f-927b2557fb5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/grover\n"
          ]
        }
      ],
      "source": [
        "# Navigate to the working folder.\n",
        "%cd grover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h08GD5foTRW",
        "outputId": "4138c9f6-e355-461b-fbda-aabb212fa6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/MyDrive/grover\n",
            "Installing collected packages: grover\n",
            "  Running setup.py develop for grover\n",
            "Successfully installed grover-1.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install the forked repository.\n",
        "!pip install -e ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bp-kelley/descriptastorus"
      ],
      "metadata": {
        "id": "KgrWl-EsztNW",
        "outputId": "470ab00d-c4aa-403d-d83c-c41612209faf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'descriptastorus' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dai45voQm4yp",
        "outputId": "4c5f7951-4c18-4e00-d658-5e4abcb93934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepchem in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Requirement already satisfied: scipy<1.9 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.8.1)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from deepchem) (2023.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install deepchem and descriptastorus.\n",
        "!pip install deepchem\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/grover/descriptastorus"
      ],
      "metadata": {
        "id": "cFcTSzPS0ROk",
        "outputId": "ec5f0928-af71-402d-ff34-c97559503aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./descriptastorus\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas_flavor in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (0.6.0)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (2023.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (1.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas_flavor->descriptastorus==2.5.0.23) (1.5.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas_flavor->descriptastorus==2.5.0.23) (2023.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->descriptastorus==2.5.0.23) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (2023.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray->pandas_flavor->descriptastorus==2.5.0.23) (23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (1.16.0)\n",
            "Building wheels for collected packages: descriptastorus\n",
            "  Building wheel for descriptastorus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for descriptastorus: filename=descriptastorus-2.5.0.23-py3-none-any.whl size=1083537 sha256=e9afdd74b9f765e179cbac608caeedaf4d550c4a492471bf1add8fa953dad68e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wd_tv_45/wheels/33/a7/79/a735914259445b357d6b62ac74806fd7d62f69c1305e4dd5fe\n",
            "Successfully built descriptastorus\n",
            "Installing collected packages: descriptastorus\n",
            "  Attempting uninstall: descriptastorus\n",
            "    Found existing installation: descriptastorus 2.5.0.23\n",
            "    Uninstalling descriptastorus-2.5.0.23:\n",
            "      Successfully uninstalled descriptastorus-2.5.0.23\n",
            "Successfully installed descriptastorus-2.5.0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRtkZuD23IVu"
      },
      "source": [
        "## Extracting semantic motif labels\n",
        "The semantic motif label is extracted by `scripts/save_feature.py` with feature generator `fgtasklabel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "altfeS6Dlfa-",
        "outputId": "4d706914-9d99-4d66-f1ee-c6e64dd7eefc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 5970/5970 [00:12<00:00, 472.47it/s]\n"
          ]
        }
      ],
      "source": [
        "!python scripts/save_features.py --data_path exampledata/pretrain/tryout.csv  \\\n",
        "                                --save_path exampledata/pretrain/tryout.npz   \\\n",
        "                                --features_generator fgtasklabel \\\n",
        "                                --restart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPiSHz7m3UME"
      },
      "source": [
        "## Extracting atom/bond contextual properties (vocabulary)\n",
        "The atom/bond Contextual Property (Vocabulary) is extracted by `scripts/build_vocab.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HJGcjwSlqey",
        "outputId": "c995a2c3-0ee8-474f-db55-dac98706e066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building atom vocab from file: exampledata/pretrain/tryout.csv\n",
            "50000it [00:03, 12577.14it/s]\n",
            "atom vocab size 324\n",
            "Building bond vocab from file: exampledata/pretrain/tryout.csv\n",
            "50000it [00:21, 2334.97it/s]\n",
            "bond vocab size 353\n"
          ]
        }
      ],
      "source": [
        "!python scripts/build_vocab.py --data_path exampledata/pretrain/tryout.csv  \\\n",
        "                             --vocab_save_folder exampledata/pretrain  \\\n",
        "                             --dataset_name tryout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s_crxb33hzD"
      },
      "source": [
        "## Splitting the data\n",
        "To accelerate the data loading and reduce the memory cost in the multi-gpu pretraining scenario, the unlabelled molecular data need to be spilt into several parts using `scripts/split_data.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofBiGAV8nhFE",
        "outputId": "95147c34-2147-46fc-f29b-f3fd66bee21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files: 60\n"
          ]
        }
      ],
      "source": [
        "!python scripts/split_data.py --data_path exampledata/pretrain/tryout.csv  \\\n",
        "                             --features_path exampledata/pretrain/tryout.npz  \\\n",
        "                             --sample_per_file 100  \\\n",
        "                             --output_path exampledata/pretrain/tryout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7tPSzcd4Iu4"
      },
      "source": [
        "## Running Pretraining on Single GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBRyJjD4oijD",
        "outputId": "4317a9a8-09b4-46f5-a376-ab72902cab8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n",
            "Namespace(parser_name='pretrain', cuda=True, enable_multi_gpu=False, data_path='exampledata/pretrain/tryout', fg_label_path=None, atom_vocab_path='exampledata/pretrain/tryout_atom_vocab.pkl', bond_vocab_path='exampledata/pretrain/tryout_bond_vocab.pkl', embedding_output_type='both', save_dir='model/tryout', save_interval=9999999999, hidden_size=100, bias=False, depth=5, dropout=0.1, activation='PReLU', undirected=False, weight_decay=1e-07, num_attn_head=1, num_mt_block=1, dist_coff=0.1, backbone='gtrans', epochs=3, batch_size=32, warmup_epochs=2.0, init_lr=0.0002, max_lr=0.0004, final_lr=0.0001, bond_drop_rate=0, dense=False, fine_tune_coff=1, no_cache=True)\n",
            "Loading data\n",
            "Loading data:\n",
            "Number of files: 60\n",
            "Number of samples: 5970\n",
            "Samples/file: 100\n",
            "Splitting data with seed 0.\n",
            "Total size = 5,970 | train size = 5,400 | val size = 570\n",
            "atom vocab size: 324, bond vocab size: 353, Number of FG tasks: 85\n",
            "Pre-loaded test data: 6\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "No checkpoint found %d\n",
            "GROVEREmbedding(\n",
            "  (encoders): GTransEncoder(\n",
            "    (edge_blocks): ModuleList(\n",
            "      (0): MTBlock(\n",
            "        (heads): ModuleList(\n",
            "          (0): Head(\n",
            "            (mpn_q): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_k): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_v): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "        (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "        (attn): MultiHeadedAttention(\n",
            "          (linear_layers): ModuleList(\n",
            "            (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "          )\n",
            "          (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (attention): Attention()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "        (sublayer): SublayerConnection(\n",
            "          (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (node_blocks): ModuleList(\n",
            "      (0): MTBlock(\n",
            "        (heads): ModuleList(\n",
            "          (0): Head(\n",
            "            (mpn_q): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_k): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_v): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "        (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "        (attn): MultiHeadedAttention(\n",
            "          (linear_layers): ModuleList(\n",
            "            (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "          )\n",
            "          (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (attention): Attention()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "        (sublayer): SublayerConnection(\n",
            "          (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (atom_from_atom_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (atom_from_bond_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (bond_from_atom_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (bond_from_bond_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (act_func_node): PReLU(num_parameters=1)\n",
            "    (act_func_edge): PReLU(num_parameters=1)\n",
            "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "Total parameters: 768614\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "Epoch: 0001 loss_train: 14.146917 loss_val: 7.921577 loss_val_av: 3.171643 loss_val_bv: 3.560064 loss_val_fg: 1.189870 cur_lr: 0.00030 t_time: 32.2825s v_time: 3.0642s d_time: 0.0000s\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "Epoch: 0002 loss_train: 6.902597 loss_val: 3.928608 loss_val_av: 1.162305 loss_val_bv: 1.792507 loss_val_fg: 0.973796 cur_lr: 0.00039 t_time: 32.4912s v_time: 2.8249s d_time: 0.0000s\n",
            "EP:3 Model Saved on: model/tryout/model.ep3\n",
            "Total Time: 71.236\n"
          ]
        }
      ],
      "source": [
        "!python main.py pretrain \\\n",
        "               --data_path exampledata/pretrain/tryout \\\n",
        "               --save_dir model/tryout \\\n",
        "               --atom_vocab_path exampledata/pretrain/tryout_atom_vocab.pkl \\\n",
        "               --bond_vocab_path exampledata/pretrain/tryout_bond_vocab.pkl \\\n",
        "               --batch_size 32 \\\n",
        "               --dropout 0.1 \\\n",
        "               --depth 5 \\\n",
        "               --num_attn_head 1 \\\n",
        "               --hidden_size 100 \\\n",
        "               --epochs 3 \\\n",
        "               --init_lr 0.0002 \\\n",
        "               --max_lr 0.0004 \\\n",
        "               --final_lr 0.0001 \\\n",
        "               --weight_decay 0.0000001 \\\n",
        "               --activation PReLU \\\n",
        "               --backbone gtrans \\\n",
        "               --embedding_output_type both"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t485Nwxt4QXL"
      },
      "source": [
        "# Training and Finetuning\n",
        "\n",
        "##Extracting Molecular Features\n",
        "\n",
        "Given a labelled molecular dataset, it is possible to extract the additional molecular features in order to train & finetune the model from the existing pretrained model. The feature matrix is stored as `.npz`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4KUfRvFomCG",
        "outputId": "45c3ff49-6859-4e64-aa1f-1d2175adf538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 2968/2968 [03:13<00:00, 15.31it/s]\n"
          ]
        }
      ],
      "source": [
        "!python scripts/save_features.py --data_path exampledata/finetune/hERG.csv \\\n",
        "                                --save_path exampledata/finetune/hERG.npz \\\n",
        "                                --features_generator rdkit_2d_normalized \\\n",
        "                                --restart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8DzWh_o4bvl"
      },
      "source": [
        "## Finetuning with existing data\n",
        "Given the labelled dataset and the molecular features, we can use `finetune` function to finetune the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYrbgvCmpNrX",
        "outputId": "e064cf3a-9afc-4ab4-cb82-77cf622e334a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n",
            "Fold 0\n",
            "Loading data\n",
            "Number of tasks = 1\n",
            "Splitting data with seed 0\n",
            "100% 2968/2968 [00:02<00:00, 1181.31it/s]\n",
            "Total scaffolds = 1,422 | train scaffolds = 1,106 | val scaffolds = 122 | test scaffolds = 194\n",
            "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([1.]), array([4])), (array([1.]), array([1])), (array([1.]), array([1])), (array([0.]), array([1])), (array([1.]), array([1])), (array([0.]), array([1])), (array([1.]), array([1])), (array([0.]), array([2])), (array([0.]), array([1])), (array([0.]), array([2]))]\n",
            "Class sizes\n",
            "Activity 0: 35.58%, 1: 64.42%\n",
            "Total size = 2,968 | train size = 2,374 | val size = 296 | test size = 298\n",
            "Loading model 0 from model/tryout/model.ep3\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
            "GroverFinetuneTask(\n",
            "  (grover): GROVEREmbedding(\n",
            "    (encoders): GTransEncoder(\n",
            "      (edge_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (node_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (atom_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (atom_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (act_func_node): PReLU(num_parameters=1)\n",
            "      (act_func_edge): PReLU(num_parameters=1)\n",
            "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (readout): Readout()\n",
            "  (mol_atom_from_atom_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (mol_atom_from_bond_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Number of parameters = 889,418\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0000 loss_train: 1.191383 loss_val: 0.583111 auc_val: 0.8244 cur_lr: 0.00059 t_time: 13.2435s v_time: 2.0069s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0001 loss_train: 1.082111 loss_val: 0.575425 auc_val: 0.8463 cur_lr: 0.00099 t_time: 11.7034s v_time: 2.2011s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0002 loss_train: 0.988432 loss_val: 0.562300 auc_val: 0.8465 cur_lr: 0.00074 t_time: 12.0035s v_time: 2.2878s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0003 loss_train: 0.925805 loss_val: 0.556260 auc_val: 0.8426 cur_lr: 0.00055 t_time: 11.7806s v_time: 2.3971s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0004 loss_train: 0.887010 loss_val: 0.555485 auc_val: 0.8457 cur_lr: 0.00041 t_time: 12.0411s v_time: 2.0624s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0005 loss_train: 0.840211 loss_val: 0.550538 auc_val: 0.8396 cur_lr: 0.00031 t_time: 12.6185s v_time: 1.5296s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0006 loss_train: 0.815271 loss_val: 0.549942 auc_val: 0.8393 cur_lr: 0.00023 t_time: 13.0152s v_time: 1.3558s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0007 loss_train: 0.790261 loss_val: 0.549278 auc_val: 0.8382 cur_lr: 0.00017 t_time: 14.7756s v_time: 2.5219s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0008 loss_train: 0.780336 loss_val: 0.551562 auc_val: 0.8395 cur_lr: 0.00013 t_time: 14.1265s v_time: 1.8963s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0009 loss_train: 0.769104 loss_val: 0.548219 auc_val: 0.8352 cur_lr: 0.00010 t_time: 13.4866s v_time: 1.9195s\n",
            "Model 0 best validation auc = 0.846501 on epoch 2\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model 0 test auc = 0.830107\n",
            "Ensemble test auc = 0.830107\n",
            "Fold 1\n",
            "Loading data\n",
            "Number of tasks = 1\n",
            "Splitting data with seed 1\n",
            "100% 2968/2968 [00:01<00:00, 1821.51it/s]\n",
            "Total scaffolds = 1,422 | train scaffolds = 1,158 | val scaffolds = 113 | test scaffolds = 151\n",
            "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([1.]), array([2])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([0.]), array([2])), (array([0.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([2])), (array([0.]), array([1]))]\n",
            "Class sizes\n",
            "Activity 0: 35.58%, 1: 64.42%\n",
            "Total size = 2,968 | train size = 2,374 | val size = 296 | test size = 298\n",
            "Loading model 0 from model/tryout/model.ep3\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
            "GroverFinetuneTask(\n",
            "  (grover): GROVEREmbedding(\n",
            "    (encoders): GTransEncoder(\n",
            "      (edge_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (node_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (atom_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (atom_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (act_func_node): PReLU(num_parameters=1)\n",
            "      (act_func_edge): PReLU(num_parameters=1)\n",
            "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (readout): Readout()\n",
            "  (mol_atom_from_atom_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (mol_atom_from_bond_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Number of parameters = 889,418\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0000 loss_train: 1.201598 loss_val: 0.576763 auc_val: 0.8730 cur_lr: 0.00059 t_time: 13.6729s v_time: 1.3765s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0001 loss_train: 1.070128 loss_val: 0.563976 auc_val: 0.8874 cur_lr: 0.00099 t_time: 13.5584s v_time: 1.5194s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0002 loss_train: 0.963119 loss_val: 0.554929 auc_val: 0.9000 cur_lr: 0.00074 t_time: 12.8013s v_time: 2.0119s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0003 loss_train: 0.887403 loss_val: 0.543951 auc_val: 0.9044 cur_lr: 0.00055 t_time: 11.9894s v_time: 2.3306s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0004 loss_train: 0.841094 loss_val: 0.540243 auc_val: 0.9044 cur_lr: 0.00041 t_time: 11.9039s v_time: 2.3049s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0005 loss_train: 0.819242 loss_val: 0.538579 auc_val: 0.9059 cur_lr: 0.00031 t_time: 11.6950s v_time: 2.3532s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0006 loss_train: 0.801368 loss_val: 0.537502 auc_val: 0.9090 cur_lr: 0.00023 t_time: 12.1054s v_time: 2.1673s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0007 loss_train: 0.774453 loss_val: 0.536170 auc_val: 0.9080 cur_lr: 0.00017 t_time: 12.3839s v_time: 1.3513s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0008 loss_train: 0.770674 loss_val: 0.532633 auc_val: 0.9098 cur_lr: 0.00013 t_time: 13.5603s v_time: 1.4186s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0009 loss_train: 0.754781 loss_val: 0.532397 auc_val: 0.9091 cur_lr: 0.00010 t_time: 13.7712s v_time: 1.3527s\n",
            "Model 0 best validation auc = 0.909767 on epoch 8\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model 0 test auc = 0.927141\n",
            "Ensemble test auc = 0.927141\n",
            "Fold 2\n",
            "Loading data\n",
            "Number of tasks = 1\n",
            "Splitting data with seed 2\n",
            "100% 2968/2968 [00:02<00:00, 1283.01it/s]\n",
            "Total scaffolds = 1,422 | train scaffolds = 1,132 | val scaffolds = 129 | test scaffolds = 161\n",
            "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([0.]), array([4])), (array([0.]), array([1])), (array([1.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([1.]), array([2])), (array([0.]), array([3])), (array([0.]), array([1])), (array([1.]), array([1])), (array([0.]), array([1]))]\n",
            "Class sizes\n",
            "Activity 0: 35.58%, 1: 64.42%\n",
            "Total size = 2,968 | train size = 2,374 | val size = 296 | test size = 298\n",
            "Loading model 0 from model/tryout/model.ep3\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
            "GroverFinetuneTask(\n",
            "  (grover): GROVEREmbedding(\n",
            "    (encoders): GTransEncoder(\n",
            "      (edge_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (node_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (atom_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (atom_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (act_func_node): PReLU(num_parameters=1)\n",
            "      (act_func_edge): PReLU(num_parameters=1)\n",
            "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (readout): Readout()\n",
            "  (mol_atom_from_atom_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (mol_atom_from_bond_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Number of parameters = 889,418\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0000 loss_train: 1.192317 loss_val: 0.638626 auc_val: 0.7388 cur_lr: 0.00059 t_time: 12.1175s v_time: 1.7653s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0001 loss_train: 1.049708 loss_val: 0.633743 auc_val: 0.7462 cur_lr: 0.00099 t_time: 12.5025s v_time: 1.3817s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0002 loss_train: 0.932790 loss_val: 0.626003 auc_val: 0.7662 cur_lr: 0.00074 t_time: 13.5922s v_time: 1.4549s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0003 loss_train: 0.904987 loss_val: 0.622769 auc_val: 0.7570 cur_lr: 0.00055 t_time: 13.7718s v_time: 1.4215s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0004 loss_train: 0.830245 loss_val: 0.618112 auc_val: 0.7690 cur_lr: 0.00041 t_time: 13.9219s v_time: 1.4559s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0005 loss_train: 0.813402 loss_val: 0.614308 auc_val: 0.7741 cur_lr: 0.00031 t_time: 14.4920s v_time: 1.4411s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0006 loss_train: 0.794307 loss_val: 0.615796 auc_val: 0.7756 cur_lr: 0.00023 t_time: 14.2723s v_time: 1.3905s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0007 loss_train: 0.772467 loss_val: 0.612858 auc_val: 0.7818 cur_lr: 0.00017 t_time: 14.3497s v_time: 1.5170s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0008 loss_train: 0.751573 loss_val: 0.613043 auc_val: 0.7847 cur_lr: 0.00013 t_time: 17.0441s v_time: 1.4062s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0009 loss_train: 0.744731 loss_val: 0.612801 auc_val: 0.7867 cur_lr: 0.00010 t_time: 14.3176s v_time: 1.8704s\n",
            "Model 0 best validation auc = 0.786667 on epoch 9\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model 0 test auc = 0.843812\n",
            "Ensemble test auc = 0.843812\n",
            "3-fold cross validation\n",
            "Seed 0 ==> test auc = 0.830107\n",
            "Seed 1 ==> test auc = 0.927141\n",
            "Seed 2 ==> test auc = 0.843812\n",
            "overall_scaffold_balanced_test_auc=0.867020\n",
            "std=0.042879\n"
          ]
        }
      ],
      "source": [
        "!python main.py finetune --data_path exampledata/finetune/hERG.csv \\\n",
        "                        --features_path exampledata/finetune/hERG.npz \\\n",
        "                        --save_dir model/finetune/hERG/ \\\n",
        "                        --checkpoint_path model/tryout/model.ep3 \\\n",
        "                        --dataset_type classification \\\n",
        "                        --split_type scaffold_balanced \\\n",
        "                        --ensemble_size 1 \\\n",
        "                        --num_folds 3 \\\n",
        "                        --no_features_scaling \\\n",
        "                        --ffn_hidden_size 200 \\\n",
        "                        --batch_size 32 \\\n",
        "                        --epochs 10 \\\n",
        "                        --init_lr 0.00015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgrgiGhH4pDJ"
      },
      "source": [
        "# Predicting output\n",
        "\n",
        "## Extracting molecular features\n",
        "\n",
        "If the finetuned model uses the molecular feature as input, we need to generate the molecular feature for the target molecules as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1SjXioJpy6E",
        "outputId": "02099d62-509a-4723-ea84-9ab75cbb084a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 2968/2968 [03:08<00:00, 15.73it/s]\n"
          ]
        }
      ],
      "source": [
        "!python scripts/save_features.py --data_path exampledata/finetune/hERG.csv \\\n",
        "                                --save_path exampledata/finetune/hERG.npz \\\n",
        "                                --features_generator rdkit_2d_normalized \\\n",
        "                                --restart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uu-qkHJ4ygI"
      },
      "source": [
        "## Predicting output with the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH6Q7l0bp_NW",
        "outputId": "ae73180a-04c3-42f8-d1cd-fb06a7af3b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n",
            "Loading training args\n",
            "Loading data\n",
            "Validating SMILES\n",
            "Test size = 2,968\n",
            "Predicting...\n",
            "\r  0% 0/6 [00:00<?, ?it/s]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 17% 1/6 [00:16<01:20, 16.19s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            " 33% 2/6 [00:29<00:58, 14.70s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            " 50% 3/6 [00:42<00:41, 13.91s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            " 67% 4/6 [00:55<00:27, 13.58s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            " 83% 5/6 [01:09<00:13, 13.67s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "100% 6/6 [01:23<00:00, 13.94s/it]\n",
            "Saving predictions to data_pre.csv\n"
          ]
        }
      ],
      "source": [
        "!python main.py predict --data_path exampledata/finetune/hERG.csv \\\n",
        "               --features_path exampledata/finetune/hERG.npz \\\n",
        "               --checkpoint_dir ./model \\\n",
        "               --no_features_scaling \\\n",
        "               --output data_pre.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9FW9hHP-FCx"
      },
      "source": [
        "## Output\n",
        "\n",
        "The output will be saved in a file called `data_pre.csv`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/grover/data_pre.csv\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "EKbPp8qeAc8Y",
        "outputId": "e07853c2-4cfa-4279-c74f-7778fb2c7a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Unnamed: 0  Activity\n",
              "0  O=C1NCCN1CCN1CCC(c2cn(-c3ccc(F)cc3)c3ccc(Cl)cc...  0.937507\n",
              "1  O=C(CCCN1CC=C(n2c(=O)[nH]c3ccccc32)CC1)c1ccc(F...  0.901423\n",
              "2  COc1ccc(CCN(C)CCCC(C#N)(c2ccc(OC)c(OC)c2)C(C)C...  0.865107\n",
              "3  CCCCN(CCCC)CCC(O)c1cc2c(Cl)cc(Cl)cc2c2cc(C(F)(...  0.893256\n",
              "4       CCOC(=O)N1CCC(=C2c3ccc(Cl)cc3CCc3cccnc32)CC1  0.857232\n",
              "5  O=C1NCCN1CCN1CCC(c2cn(-c3ccc(F)cc3)c3ccc(Cl)cc...  0.937507\n",
              "6  COC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[...  0.527712\n",
              "7     O=C1Cc2cc(CCN3CCN(c4nsc5ccccc45)CC3)c(Cl)cc2N1  0.900057\n",
              "8  COc1ccc(Cl)cc1C(=O)NCCc1ccc(S(=O)(=O)NC(=O)NC2...  0.628845\n",
              "9  CCCc1nn(C)c2c(=O)[nH]c(-c3cc(S(=O)(=O)N4CCN(C)...  0.364751"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-09a4e277-b643-455f-a439-6c27c9e58c1f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>O=C1NCCN1CCN1CCC(c2cn(-c3ccc(F)cc3)c3ccc(Cl)cc...</td>\n",
              "      <td>0.937507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=C(CCCN1CC=C(n2c(=O)[nH]c3ccccc32)CC1)c1ccc(F...</td>\n",
              "      <td>0.901423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COc1ccc(CCN(C)CCCC(C#N)(c2ccc(OC)c(OC)c2)C(C)C...</td>\n",
              "      <td>0.865107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CCCCN(CCCC)CCC(O)c1cc2c(Cl)cc(Cl)cc2c2cc(C(F)(...</td>\n",
              "      <td>0.893256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CCOC(=O)N1CCC(=C2c3ccc(Cl)cc3CCc3cccnc32)CC1</td>\n",
              "      <td>0.857232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>O=C1NCCN1CCN1CCC(c2cn(-c3ccc(F)cc3)c3ccc(Cl)cc...</td>\n",
              "      <td>0.937507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>COC(=O)[C@H]1[C@@H](OC(=O)c2ccccc2)C[C@@H]2CC[...</td>\n",
              "      <td>0.527712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>O=C1Cc2cc(CCN3CCN(c4nsc5ccccc45)CC3)c(Cl)cc2N1</td>\n",
              "      <td>0.900057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>COc1ccc(Cl)cc1C(=O)NCCc1ccc(S(=O)(=O)NC(=O)NC2...</td>\n",
              "      <td>0.628845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CCCc1nn(C)c2c(=O)[nH]c(-c3cc(S(=O)(=O)N4CCN(C)...</td>\n",
              "      <td>0.364751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09a4e277-b643-455f-a439-6c27c9e58c1f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-55b2c8c7-866e-4161-aeac-ec550b40583a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55b2c8c7-866e-4161-aeac-ec550b40583a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-55b2c8c7-866e-4161-aeac-ec550b40583a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09a4e277-b643-455f-a439-6c27c9e58c1f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09a4e277-b643-455f-a439-6c27c9e58c1f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}