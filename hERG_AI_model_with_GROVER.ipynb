{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jahan08/Advanced-AI-algorithm-practice-hERG-data/blob/main/hERG_AI_model_with_GROVER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AGV4FM3znB2"
      },
      "source": [
        "# Introduction to GROVER\n",
        "\n",
        "In this tutorial, we will go over what Grover is, and how to get it up and running.\n",
        "\n",
        "GROVER, or, Graph Representation frOm selfsuperVised mEssage passing tRansformer, is a novel framework proposed by Tencent AI Lab. GROVER utilizes self-supervised tasks in the node, edge and graph level in order to learn rich structural and semantic information of molecules from large unlabelled molecular datasets. GROVER integrates Message Passing Networks into a Transformer-style architecture to deliver more expressive molecular encoding.\n",
        "\n",
        "Reference Paper: [Rong, Yu, et al. \"Grover: Self-supervised message passing transformer on large-scale molecular data.\" Advances in Neural Information Processing Systems (2020).](https://drug.ai.tencent.com/publications/GROVER.pdf)\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/Introduction_to_GROVER.ipynb)\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run DeepChem within Colab, you'll need to run the following installation commands. This will take about 5 minutes to run to completion and install your environment. You can of course run this tutorial locally if you prefer. In that case, don't run these cells since they will download and install Anaconda on your local machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gySfXfHP15A2"
      },
      "source": [
        "## Import and Setup required modules.\n",
        "We will first clone the repository onto the preferred platform, then install it as a library. We will also import deepchem and install descriptastorus.\n",
        "\n",
        "NOTE: The [original GROVER repository](https://github.com/tencent-ailab/grover) does not contain a `setup.py` file, thus we are currently using a fork which does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc9dofL-kbyf",
        "outputId": "416f4817-1de2-43be-a7d8-e4fa00e7ecfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grover'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 206 (delta 11), reused 4 (delta 4), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (206/206), 2.21 MiB | 2.59 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the forked repository.\n",
        "#%cd drive/MyDrive\n",
        "!git clone https://github.com/atreyamaj/grover.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsAZo_sz5nRv",
        "outputId": "1158783e-6746-47c9-cdff-ae3ef25ed2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/grover\n"
          ]
        }
      ],
      "source": [
        "# Navigate to the working folder.\n",
        "%cd grover"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h08GD5foTRW",
        "outputId": "ec3030ff-d597-4b6a-bc09-993c97af1673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/grover\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: grover\n",
            "  Running setup.py develop for grover\n",
            "Successfully installed grover-1.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install the forked repository.\n",
        "!pip install -e ./"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bp-kelley/descriptastorus"
      ],
      "metadata": {
        "id": "KgrWl-EsztNW",
        "outputId": "2615bbf4-e15f-4aa2-a883-c2a7c8f10cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'descriptastorus'...\n",
            "remote: Enumerating objects: 2045, done.\u001b[K\n",
            "remote: Counting objects: 100% (298/298), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 2045 (delta 186), reused 286 (delta 180), pack-reused 1747\u001b[K\n",
            "Receiving objects: 100% (2045/2045), 52.75 MiB | 11.84 MiB/s, done.\n",
            "Resolving deltas: 100% (944/944), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dai45voQm4yp",
        "outputId": "b6e07e6c-2f6a-45af-f44f-4702e91cc15d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit (from deepchem)\n",
            "  Downloading rdkit-2023.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2023.3.post1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (9.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: scipy, rdkit, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.2\n",
            "    Uninstalling scipy-1.11.2:\n",
            "      Successfully uninstalled scipy-1.11.2\n",
            "Successfully installed deepchem-2.7.1 rdkit-2023.3.3 scipy-1.8.1\n"
          ]
        }
      ],
      "source": [
        "# Install deepchem and descriptastorus.\n",
        "!pip install deepchem\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/grover/descriptastorus"
      ],
      "metadata": {
        "id": "cFcTSzPS0ROk",
        "outputId": "d2c8d048-b90c-4c7b-f0dd-0cc0289f149d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./descriptastorus\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas_flavor in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (0.6.0)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (2023.3.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (1.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from descriptastorus==2.5.0.23) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas_flavor->descriptastorus==2.5.0.23) (1.5.3)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from pandas_flavor->descriptastorus==2.5.0.23) (2023.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->descriptastorus==2.5.0.23) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (2023.3.post1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from xarray->pandas_flavor->descriptastorus==2.5.0.23) (23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_flavor->descriptastorus==2.5.0.23) (1.16.0)\n",
            "Building wheels for collected packages: descriptastorus\n",
            "  Building wheel for descriptastorus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for descriptastorus: filename=descriptastorus-2.5.0.23-py3-none-any.whl size=1083538 sha256=c9225b77ab1b04c334a56fc406b87a088d61eed2a7a1b5b1b2c89c13b5ec8367\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lge56n9g/wheels/33/a7/79/a735914259445b357d6b62ac74806fd7d62f69c1305e4dd5fe\n",
            "Successfully built descriptastorus\n",
            "Installing collected packages: descriptastorus\n",
            "  Attempting uninstall: descriptastorus\n",
            "    Found existing installation: descriptastorus 2.5.0.23\n",
            "    Uninstalling descriptastorus-2.5.0.23:\n",
            "      Successfully uninstalled descriptastorus-2.5.0.23\n",
            "Successfully installed descriptastorus-2.5.0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRtkZuD23IVu"
      },
      "source": [
        "## Extracting semantic motif labels\n",
        "The semantic motif label is extracted by `scripts/save_feature.py` with feature generator `fgtasklabel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "altfeS6Dlfa-",
        "outputId": "8fb625f4-b41d-49e7-e081-536401c4a720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 5970/5970 [00:15<00:00, 381.77it/s]\n"
          ]
        }
      ],
      "source": [
        "!python scripts/save_features.py --data_path exampledata/pretrain/tryout.csv  \\\n",
        "                                --save_path exampledata/pretrain/tryout.npz   \\\n",
        "                                --features_generator fgtasklabel \\\n",
        "                                --restart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPiSHz7m3UME"
      },
      "source": [
        "## Extracting atom/bond contextual properties (vocabulary)\n",
        "The atom/bond Contextual Property (Vocabulary) is extracted by `scripts/build_vocab.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HJGcjwSlqey",
        "outputId": "f4181a4b-c2eb-429c-e8e1-4595d8db76bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building atom vocab from file: exampledata/pretrain/tryout.csv\n",
            "50000it [00:03, 13801.26it/s]\n",
            "atom vocab size 324\n",
            "Building bond vocab from file: exampledata/pretrain/tryout.csv\n",
            "50000it [00:21, 2332.31it/s]\n",
            "bond vocab size 353\n"
          ]
        }
      ],
      "source": [
        "!python scripts/build_vocab.py --data_path exampledata/pretrain/tryout.csv  \\\n",
        "                             --vocab_save_folder exampledata/pretrain  \\\n",
        "                             --dataset_name tryout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s_crxb33hzD"
      },
      "source": [
        "## Splitting the data\n",
        "To accelerate the data loading and reduce the memory cost in the multi-gpu pretraining scenario, the unlabelled molecular data need to be spilt into several parts using `scripts/split_data.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofBiGAV8nhFE",
        "outputId": "cf6984cd-cc09-4eb3-8b70-ab3ff7ccbb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files: 60\n"
          ]
        }
      ],
      "source": [
        "!python scripts/split_data.py --data_path exampledata/pretrain/tryout.csv  \\\n",
        "                             --features_path exampledata/pretrain/tryout.npz  \\\n",
        "                             --sample_per_file 100  \\\n",
        "                             --output_path exampledata/pretrain/tryout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7tPSzcd4Iu4"
      },
      "source": [
        "## Running Pretraining on Single GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBRyJjD4oijD",
        "outputId": "d3f3596a-adc2-4454-836e-405a42c802a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n",
            "Namespace(parser_name='pretrain', cuda=True, enable_multi_gpu=False, data_path='exampledata/pretrain/tryout', fg_label_path=None, atom_vocab_path='exampledata/pretrain/tryout_atom_vocab.pkl', bond_vocab_path='exampledata/pretrain/tryout_bond_vocab.pkl', embedding_output_type='both', save_dir='model/tryout', save_interval=9999999999, hidden_size=100, bias=False, depth=5, dropout=0.1, activation='PReLU', undirected=False, weight_decay=1e-07, num_attn_head=1, num_mt_block=1, dist_coff=0.1, backbone='gtrans', epochs=3, batch_size=32, warmup_epochs=2.0, init_lr=0.0002, max_lr=0.0004, final_lr=0.0001, bond_drop_rate=0, dense=False, fine_tune_coff=1, no_cache=True)\n",
            "Loading data\n",
            "Loading data:\n",
            "Number of files: 60\n",
            "Number of samples: 5970\n",
            "Samples/file: 100\n",
            "Splitting data with seed 0.\n",
            "Total size = 5,970 | train size = 5,400 | val size = 570\n",
            "atom vocab size: 324, bond vocab size: 353, Number of FG tasks: 85\n",
            "Pre-loaded test data: 6\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "No checkpoint found %d\n",
            "GROVEREmbedding(\n",
            "  (encoders): GTransEncoder(\n",
            "    (edge_blocks): ModuleList(\n",
            "      (0): MTBlock(\n",
            "        (heads): ModuleList(\n",
            "          (0): Head(\n",
            "            (mpn_q): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_k): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_v): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "        (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "        (attn): MultiHeadedAttention(\n",
            "          (linear_layers): ModuleList(\n",
            "            (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "          )\n",
            "          (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (attention): Attention()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "        (sublayer): SublayerConnection(\n",
            "          (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (node_blocks): ModuleList(\n",
            "      (0): MTBlock(\n",
            "        (heads): ModuleList(\n",
            "          (0): Head(\n",
            "            (mpn_q): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_k): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "            (mpn_v): MPNEncoder(\n",
            "              (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "              (act_func): PReLU(num_parameters=1)\n",
            "              (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "        (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "        (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "        (attn): MultiHeadedAttention(\n",
            "          (linear_layers): ModuleList(\n",
            "            (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "          )\n",
            "          (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (attention): Attention()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "        (sublayer): SublayerConnection(\n",
            "          (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "      (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "      (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (act_func): PReLU(num_parameters=1)\n",
            "    )\n",
            "    (atom_from_atom_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (atom_from_bond_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (bond_from_atom_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (bond_from_bond_sublayer): SublayerConnection(\n",
            "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (act_func_node): PReLU(num_parameters=1)\n",
            "    (act_func_edge): PReLU(num_parameters=1)\n",
            "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "Total parameters: 768614\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "Epoch: 0001 loss_train: 14.204253 loss_val: 7.918376 loss_val_av: 3.168683 loss_val_bv: 3.545966 loss_val_fg: 1.203728 cur_lr: 0.00030 t_time: 32.7226s v_time: 2.7763s d_time: 0.0000s\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "/content/grover/grover/data/groverdataset.py:240: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  fgroup_label = torch.Tensor([d.features for d in batch]).float()\n",
            "Epoch: 0002 loss_train: 6.855205 loss_val: 4.060300 loss_val_av: 1.281727 loss_val_bv: 1.810707 loss_val_fg: 0.967866 cur_lr: 0.00039 t_time: 33.2487s v_time: 2.9090s d_time: 0.0000s\n",
            "EP:3 Model Saved on: model/tryout/model.ep3\n",
            "Total Time: 72.134\n"
          ]
        }
      ],
      "source": [
        "!python main.py pretrain \\\n",
        "               --data_path exampledata/pretrain/tryout \\\n",
        "               --save_dir model/tryout \\\n",
        "               --atom_vocab_path exampledata/pretrain/tryout_atom_vocab.pkl \\\n",
        "               --bond_vocab_path exampledata/pretrain/tryout_bond_vocab.pkl \\\n",
        "               --batch_size 32 \\\n",
        "               --dropout 0.1 \\\n",
        "               --depth 5 \\\n",
        "               --num_attn_head 1 \\\n",
        "               --hidden_size 100 \\\n",
        "               --epochs 3 \\\n",
        "               --init_lr 0.0002 \\\n",
        "               --max_lr 0.0004 \\\n",
        "               --final_lr 0.0001 \\\n",
        "               --weight_decay 0.0000001 \\\n",
        "               --activation PReLU \\\n",
        "               --backbone gtrans \\\n",
        "               --embedding_output_type both"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t485Nwxt4QXL"
      },
      "source": [
        "# Training and Finetuning\n",
        "\n",
        "##Extracting Molecular Features\n",
        "\n",
        "Given a labelled molecular dataset, it is possible to extract the additional molecular features in order to train & finetune the model from the existing pretrained model. The feature matrix is stored as `.npz`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4KUfRvFomCG",
        "outputId": "26ece757-6ac6-4255-99b4-4a1543be903e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 2968/2968 [03:21<00:00, 14.74it/s]\n"
          ]
        }
      ],
      "source": [
        "!python scripts/save_features.py --data_path exampledata/finetune/hERG.csv \\\n",
        "                                --save_path exampledata/finetune/hERG.npz \\\n",
        "                                --features_generator rdkit_2d_normalized \\\n",
        "                                --restart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8DzWh_o4bvl"
      },
      "source": [
        "## Finetuning with existing data\n",
        "Given the labelled dataset and the molecular features, we can use `finetune` function to finetune the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYrbgvCmpNrX",
        "outputId": "5f7e95a9-80a6-41ad-aac1-19f606bb0db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n",
            "Fold 0\n",
            "Loading data\n",
            "Number of tasks = 1\n",
            "Splitting data with seed 0\n",
            "100% 2968/2968 [00:01<00:00, 1630.21it/s]\n",
            "Total scaffolds = 1,422 | train scaffolds = 1,136 | val scaffolds = 146 | test scaffolds = 140\n",
            "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([1.]), array([1])), (array([1.]), array([2])), (array([0.]), array([1])), (array([1.]), array([1])), (array([1.]), array([2])), (array([1.]), array([1])), (array([1.]), array([2])), (array([0.]), array([1])), (array([1.]), array([2])), (array([1.]), array([4]))]\n",
            "Class sizes\n",
            "Activity 0: 35.58%, 1: 64.42%\n",
            "Total size = 2,968 | train size = 2,374 | val size = 296 | test size = 298\n",
            "Loading model 0 from model/tryout/model.ep3\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
            "GroverFinetuneTask(\n",
            "  (grover): GROVEREmbedding(\n",
            "    (encoders): GTransEncoder(\n",
            "      (edge_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (node_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (atom_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (atom_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (act_func_node): PReLU(num_parameters=1)\n",
            "      (act_func_edge): PReLU(num_parameters=1)\n",
            "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (readout): Readout()\n",
            "  (mol_atom_from_atom_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (mol_atom_from_bond_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Number of parameters = 889,418\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0000 loss_train: 1.199026 loss_val: 0.584329 auc_val: 0.7995 cur_lr: 0.00059 t_time: 16.0788s v_time: 1.2291s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0001 loss_train: 1.068245 loss_val: 0.578468 auc_val: 0.8383 cur_lr: 0.00099 t_time: 13.3546s v_time: 1.2505s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0002 loss_train: 0.944879 loss_val: 0.566425 auc_val: 0.8440 cur_lr: 0.00074 t_time: 13.4416s v_time: 1.2546s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0003 loss_train: 0.952190 loss_val: 0.562186 auc_val: 0.8589 cur_lr: 0.00055 t_time: 13.1421s v_time: 1.2437s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0004 loss_train: 0.870922 loss_val: 0.556811 auc_val: 0.8679 cur_lr: 0.00041 t_time: 12.5464s v_time: 1.8654s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0005 loss_train: 0.831633 loss_val: 0.555620 auc_val: 0.8557 cur_lr: 0.00031 t_time: 11.8525s v_time: 2.0114s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0006 loss_train: 0.826724 loss_val: 0.552833 auc_val: 0.8719 cur_lr: 0.00023 t_time: 11.2811s v_time: 2.2472s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0007 loss_train: 0.819173 loss_val: 0.551659 auc_val: 0.8753 cur_lr: 0.00017 t_time: 12.2178s v_time: 1.5865s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0008 loss_train: 0.786996 loss_val: 0.551106 auc_val: 0.8714 cur_lr: 0.00013 t_time: 13.2222s v_time: 1.5234s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0009 loss_train: 0.776806 loss_val: 0.549836 auc_val: 0.8750 cur_lr: 0.00010 t_time: 13.8056s v_time: 1.4733s\n",
            "Model 0 best validation auc = 0.875315 on epoch 7\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model 0 test auc = 0.883005\n",
            "Ensemble test auc = 0.883005\n",
            "Fold 1\n",
            "Loading data\n",
            "Number of tasks = 1\n",
            "Splitting data with seed 1\n",
            "100% 2968/2968 [00:01<00:00, 1870.55it/s]\n",
            "Total scaffolds = 1,422 | train scaffolds = 1,131 | val scaffolds = 154 | test scaffolds = 137\n",
            "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([1.]), array([1])), (array([0.66666667]), array([3])), (array([0.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([0.]), array([1])), (array([0.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([0.]), array([2]))]\n",
            "Class sizes\n",
            "Activity 0: 35.58%, 1: 64.42%\n",
            "Total size = 2,968 | train size = 2,374 | val size = 296 | test size = 298\n",
            "Loading model 0 from model/tryout/model.ep3\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
            "GroverFinetuneTask(\n",
            "  (grover): GROVEREmbedding(\n",
            "    (encoders): GTransEncoder(\n",
            "      (edge_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (node_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (atom_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (atom_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (act_func_node): PReLU(num_parameters=1)\n",
            "      (act_func_edge): PReLU(num_parameters=1)\n",
            "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (readout): Readout()\n",
            "  (mol_atom_from_atom_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (mol_atom_from_bond_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Number of parameters = 889,418\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0000 loss_train: 1.167294 loss_val: 0.620774 auc_val: 0.8616 cur_lr: 0.00059 t_time: 12.8803s v_time: 1.6710s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0001 loss_train: 1.113098 loss_val: 0.620730 auc_val: 0.8937 cur_lr: 0.00099 t_time: 12.0552s v_time: 2.0860s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0002 loss_train: 0.979284 loss_val: 0.596588 auc_val: 0.9095 cur_lr: 0.00074 t_time: 11.2772s v_time: 2.3518s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0003 loss_train: 0.899073 loss_val: 0.591225 auc_val: 0.9083 cur_lr: 0.00055 t_time: 11.6186s v_time: 1.9869s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0004 loss_train: 0.839891 loss_val: 0.588526 auc_val: 0.9045 cur_lr: 0.00041 t_time: 12.1109s v_time: 1.3570s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0005 loss_train: 0.847397 loss_val: 0.584609 auc_val: 0.9125 cur_lr: 0.00031 t_time: 13.0077s v_time: 1.3296s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0006 loss_train: 0.804514 loss_val: 0.579790 auc_val: 0.9170 cur_lr: 0.00023 t_time: 13.3563s v_time: 1.3260s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0007 loss_train: 0.801431 loss_val: 0.580089 auc_val: 0.9184 cur_lr: 0.00017 t_time: 13.6997s v_time: 1.3575s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0008 loss_train: 0.780754 loss_val: 0.577764 auc_val: 0.9200 cur_lr: 0.00013 t_time: 13.2486s v_time: 1.3452s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0009 loss_train: 0.767355 loss_val: 0.577154 auc_val: 0.9184 cur_lr: 0.00010 t_time: 13.4932s v_time: 1.3703s\n",
            "Model 0 best validation auc = 0.919978 on epoch 8\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model 0 test auc = 0.893649\n",
            "Ensemble test auc = 0.893649\n",
            "Fold 2\n",
            "Loading data\n",
            "Number of tasks = 1\n",
            "Splitting data with seed 2\n",
            "100% 2968/2968 [00:01<00:00, 1853.44it/s]\n",
            "Total scaffolds = 1,422 | train scaffolds = 1,120 | val scaffolds = 146 | test scaffolds = 156\n",
            "Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([0.]), array([1])), (array([0.]), array([1])), (array([1.]), array([4])), (array([1.]), array([1])), (array([0.]), array([1])), (array([1.]), array([4])), (array([0.]), array([3])), (array([1.]), array([1])), (array([0.]), array([2])), (array([1.]), array([2]))]\n",
            "Class sizes\n",
            "Activity 0: 35.58%, 1: 64.42%\n",
            "Total size = 2,968 | train size = 2,374 | val size = 296 | test size = 298\n",
            "Loading model 0 from model/tryout/model.ep3\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Pretrained parameter \"av_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"av_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_atom.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"bv_task_bond.linear_rev.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.readout.cached_zero_vector\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_atom_from_bond.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_atom.bias\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.weight\" cannot be found in model parameters.\n",
            "Pretrained parameter \"fg_task_all.linear_bond_from_bond.bias\" cannot be found in model parameters.\n",
            "GroverFinetuneTask(\n",
            "  (grover): GROVEREmbedding(\n",
            "    (encoders): GTransEncoder(\n",
            "      (edge_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=165, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (node_blocks): ModuleList(\n",
            "        (0): MTBlock(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (mpn_q): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_k): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "              (mpn_v): MPNEncoder(\n",
            "                (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "                (act_func): PReLU(num_parameters=1)\n",
            "                (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (act_func): PReLU(num_parameters=1)\n",
            "          (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "          (layernorm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "          (W_i): Linear(in_features=151, out_features=100, bias=False)\n",
            "          (attn): MultiHeadedAttention(\n",
            "            (linear_layers): ModuleList(\n",
            "              (0-2): 3 x Linear(in_features=100, out_features=100, bias=True)\n",
            "            )\n",
            "            (output_linear): Linear(in_features=100, out_features=100, bias=False)\n",
            "            (attention): Attention()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
            "          (sublayer): SublayerConnection(\n",
            "            (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ffn_atom_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_atom_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=251, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_atom): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (ffn_bond_from_bond): PositionwiseFeedForward(\n",
            "        (W_1): Linear(in_features=265, out_features=400, bias=True)\n",
            "        (W_2): Linear(in_features=400, out_features=100, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act_func): PReLU(num_parameters=1)\n",
            "      )\n",
            "      (atom_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (atom_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_atom_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (bond_from_bond_sublayer): SublayerConnection(\n",
            "        (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (act_func_node): PReLU(num_parameters=1)\n",
            "      (act_func_edge): PReLU(num_parameters=1)\n",
            "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (readout): Readout()\n",
            "  (mol_atom_from_atom_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (mol_atom_from_bond_ffn): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=300, out_features=200, bias=True)\n",
            "    (2): PReLU(num_parameters=1)\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Number of parameters = 889,418\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0000 loss_train: 1.205586 loss_val: 0.562724 auc_val: 0.8948 cur_lr: 0.00059 t_time: 13.2429s v_time: 1.3139s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0001 loss_train: 1.030871 loss_val: 0.547182 auc_val: 0.9121 cur_lr: 0.00099 t_time: 13.6163s v_time: 1.3432s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0002 loss_train: 0.987587 loss_val: 0.540772 auc_val: 0.9127 cur_lr: 0.00074 t_time: 13.4273s v_time: 1.3432s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0003 loss_train: 0.913391 loss_val: 0.534793 auc_val: 0.9121 cur_lr: 0.00055 t_time: 13.5115s v_time: 1.3272s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0004 loss_train: 0.854761 loss_val: 0.530384 auc_val: 0.9122 cur_lr: 0.00041 t_time: 12.9773s v_time: 1.6148s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0005 loss_train: 0.828300 loss_val: 0.527382 auc_val: 0.9157 cur_lr: 0.00031 t_time: 12.8152s v_time: 2.0040s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0006 loss_train: 0.809689 loss_val: 0.524648 auc_val: 0.9157 cur_lr: 0.00023 t_time: 12.5847s v_time: 2.1073s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0007 loss_train: 0.792369 loss_val: 0.525211 auc_val: 0.9174 cur_lr: 0.00017 t_time: 13.3364s v_time: 2.1525s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0008 loss_train: 0.766573 loss_val: 0.522314 auc_val: 0.9186 cur_lr: 0.00013 t_time: 12.9036s v_time: 2.1124s\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch: 0009 loss_train: 0.770116 loss_val: 0.524075 auc_val: 0.9152 cur_lr: 0.00010 t_time: 13.2305s v_time: 2.3414s\n",
            "Model 0 best validation auc = 0.918587 on epoch 8\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Model 0 test auc = 0.889894\n",
            "Ensemble test auc = 0.889894\n",
            "3-fold cross validation\n",
            "Seed 0 ==> test auc = 0.883005\n",
            "Seed 1 ==> test auc = 0.893649\n",
            "Seed 2 ==> test auc = 0.889894\n",
            "overall_scaffold_balanced_test_auc=0.888849\n",
            "std=0.004408\n"
          ]
        }
      ],
      "source": [
        "!python main.py finetune --data_path exampledata/finetune/hERG.csv \\\n",
        "                        --features_path exampledata/finetune/hERG.npz \\\n",
        "                        --save_dir model/finetune/hERG/ \\\n",
        "                        --checkpoint_path model/tryout/model.ep3 \\\n",
        "                        --dataset_type classification \\\n",
        "                        --split_type scaffold_balanced \\\n",
        "                        --ensemble_size 1 \\\n",
        "                        --num_folds 3 \\\n",
        "                        --no_features_scaling \\\n",
        "                        --ffn_hidden_size 200 \\\n",
        "                        --batch_size 32 \\\n",
        "                        --epochs 10 \\\n",
        "                        --init_lr 0.00015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgrgiGhH4pDJ"
      },
      "source": [
        "# Predicting output\n",
        "\n",
        "## Extracting molecular features\n",
        "\n",
        "If the finetuned model uses the molecular feature as input, we need to generate the molecular feature for the target molecules as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1SjXioJpy6E",
        "outputId": "2ac8aa87-d6c9-43a8-f8af-e9244c5e5279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 17/17 [00:01<00:00, 10.02it/s]\n"
          ]
        }
      ],
      "source": [
        "!python scripts/save_features.py --data_path exampledata/finetune/hERG-house.csv \\\n",
        "                                --save_path exampledata/finetune/hERG-house.npz \\\n",
        "                                --features_generator rdkit_2d_normalized \\\n",
        "                                --restart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uu-qkHJ4ygI"
      },
      "source": [
        "## Predicting output with the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH6Q7l0bp_NW",
        "outputId": "76198b61-7a9a-4358-941f-909604bd8202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARNING] Horovod cannot be imported; multi-GPU training is unsupported\n",
            "Loading training args\n",
            "Loading data\n",
            "Validating SMILES\n",
            "Test size = 17\n",
            "Predicting...\n",
            "\r  0% 0/3 [00:00<?, ?it/s]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 33% 1/3 [00:02<00:04,  2.46s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            " 67% 2/3 [00:02<00:01,  1.20s/it]Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
            "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n",
            "Loading pretrained parameter \"readout.cached_zero_vector\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_atom_ffn.4.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.1.bias\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.2.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.weight\".\n",
            "Loading pretrained parameter \"mol_atom_from_bond_ffn.4.bias\".\n",
            "Moving model to cuda\n",
            "100% 3/3 [00:03<00:00,  1.03s/it]\n",
            "Saving predictions to data_pre.csv\n"
          ]
        }
      ],
      "source": [
        "!python main.py predict --data_path exampledata/finetune/hERG-house.csv \\\n",
        "               --features_path exampledata/finetune/hERG-house.npz \\\n",
        "               --checkpoint_dir ./model \\\n",
        "               --no_features_scaling \\\n",
        "               --output data_pre.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9FW9hHP-FCx"
      },
      "source": [
        "## Output\n",
        "\n",
        "The output will be saved in a file called `data_pre.csv`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/grover/data_pre.csv\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "EKbPp8qeAc8Y",
        "outputId": "499e7be7-2046-43ef-de34-4cff2704ca10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Unnamed: 0  Activity\n",
              "0  OC(C1=CC=CC=C1)(C2CCN(CCC[C@@H](C3=CC=C(C(C)(C...  0.946972\n",
              "1  O=C(NCCCCN1CCN(C2=CC(C(F)(F)F)=NC(C(C)(C)C)=N2...  0.630853\n",
              "2  O=C(COC(C=CN1)=NC1=O)NC23CC4C[C@H](C2)C[C@@H](...  0.140539\n",
              "3  O=C(C1=CC(CCC(N2)=O)=C2C=C1)NC3=CC(C#CC4=NC=CC...  0.488667\n",
              "4  O=C(CN1CCCC[C@@H]1C(N)=O)C(C(N2CC)=O)=C(N(C2=O...  0.293114\n",
              "5  O=C(CN1CCCC[C@@H]1C(N)=O)C(C(N2CC)=O)=C(N(C2=O...  0.309180\n",
              "6  O=C(N)CN(C(C1=CNC(N(C2=O)CC3=CC=C(F)C=C3)=C1C(...  0.154679\n",
              "7  O=C1N(C2CCC(NC2=O)=O)CC3=C1C=CC(CNC4=NC(C(C=C5...  0.494759\n",
              "8  CC(C=C1)=C(Cl)C=C1NC2=NC(C3=CC(CN(C4=O)C5C(NC(...  0.586602\n",
              "9  O=C(CN1CCCC[C@@H]1C(N2CCOCC2)=O)C(C(N3CC)=O)=C...  0.155188"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eff47fe4-e4d3-4024-9a68-179d92e99fef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Activity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OC(C1=CC=CC=C1)(C2CCN(CCC[C@@H](C3=CC=C(C(C)(C...</td>\n",
              "      <td>0.946972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>O=C(NCCCCN1CCN(C2=CC(C(F)(F)F)=NC(C(C)(C)C)=N2...</td>\n",
              "      <td>0.630853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>O=C(COC(C=CN1)=NC1=O)NC23CC4C[C@H](C2)C[C@@H](...</td>\n",
              "      <td>0.140539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>O=C(C1=CC(CCC(N2)=O)=C2C=C1)NC3=CC(C#CC4=NC=CC...</td>\n",
              "      <td>0.488667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O=C(CN1CCCC[C@@H]1C(N)=O)C(C(N2CC)=O)=C(N(C2=O...</td>\n",
              "      <td>0.293114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>O=C(CN1CCCC[C@@H]1C(N)=O)C(C(N2CC)=O)=C(N(C2=O...</td>\n",
              "      <td>0.309180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>O=C(N)CN(C(C1=CNC(N(C2=O)CC3=CC=C(F)C=C3)=C1C(...</td>\n",
              "      <td>0.154679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>O=C1N(C2CCC(NC2=O)=O)CC3=C1C=CC(CNC4=NC(C(C=C5...</td>\n",
              "      <td>0.494759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CC(C=C1)=C(Cl)C=C1NC2=NC(C3=CC(CN(C4=O)C5C(NC(...</td>\n",
              "      <td>0.586602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>O=C(CN1CCCC[C@@H]1C(N2CCOCC2)=O)C(C(N3CC)=O)=C...</td>\n",
              "      <td>0.155188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eff47fe4-e4d3-4024-9a68-179d92e99fef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eff47fe4-e4d3-4024-9a68-179d92e99fef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eff47fe4-e4d3-4024-9a68-179d92e99fef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b54fd0b0-aa0b-4b7c-b606-96df4af15e27\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b54fd0b0-aa0b-4b7c-b606-96df4af15e27')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b54fd0b0-aa0b-4b7c-b606-96df4af15e27 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x7caa8435dab0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Values</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "import numpy as np\n",
              "from google.colab import autoviz\n",
              "\n",
              "def value_plot(df, y, figscale=1):\n",
              "  from matplotlib import pyplot as plt\n",
              "  df[y].plot(kind='line', figsize=(8 * figscale, 4 * figscale), title=y)\n",
              "  plt.gca().spines[['top', 'right']].set_visible(False)\n",
              "  plt.tight_layout()\n",
              "  return autoviz.MplChart.from_current_mpl_state()\n",
              "\n",
              "chart = value_plot(_df_0, *['Activity'], **{})\n",
              "chart"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-795a2fee-9beb-497d-aacb-30a117f4312c\">\n",
              "        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYwAAADECAYAAACMRRb/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAt6ElEQVR4nO3deVxU9frA8c8MyyCyqmwKo6KCG4IbKu6ZaaamUlZqN0vcql+L\n",
              "rXq71b2VWrfsmjcVW8zSrAw1b1m5lXsgpqKoLLIMKIiogIjs5/fHIElqgixnZnjer9e8ivHMmYd5\n",
              "MfPMec73PI9GURQFIYQQ4ha0agcghBDCPEjCEEIIUS2SMIQQQlSLJAwhhBDVIglDCCFEtUjCEEII\n",
              "US2SMIQQQlSLJAwhhBDVIglDCCFEtUjCEKIWDAYDDg4OJCUlVWv7+fPnc9ddd9VzVELUD0kYolFa\n",
              "uXIlGo2Gl19+udqPef311xkwYECV+/R6Pfn5+fj6+lZrH/PmzWPLli2VPw8ZMoRXXnml2jEIoSZJ\n",
              "GKJRWrp0Kc2bN+fTTz+lqKhI7XCEMAuSMESjc+DAAaKjo1m9ejW5ubmsW7eu8t8uXLjA448/Ttu2\n",
              "bXF0dKRjx478/PPPrFmzhvnz57N//34cHBxwcHBg9+7dpKSkoNFoSExMJDc3F3t7e3bv3l3l+Z56\n",
              "6inGjh0LVD1KmTVrFrt37+add96p3GdeXh4ODg7s3Lmzyj6efPJJxo0bV78vjBC3IAlDNDpLly4l\n",
              "KCiIkSNHMn78eJYuXQqAoiiMGzeOlJQUdu7cSV5eHps3b8bHx4fJkyczb948+vXrR35+Pvn5+Qwc\n",
              "OLDKfp2dnbnvvvv45JNPKu8rLCxk9erVhIWFXRfH8uXLGThwIC+++GLlPp2cnJg0aRIrVqyo3K6g\n",
              "oIDVq1cza9asenpFhKgeSRiiUbl48SJff/01M2bMAGDGjBns37+fI0eOcPDgQfbs2cOqVavQ6/Vo\n",
              "NBp8fX3p3LlztfcfFhbGunXryMvLAyAiIgI7Ozvuueeeau9j9uzZREREcP78eQC++uormjVrxogR\n",
              "I2rwmwpR9yRhiEbl6snuyZMnAzB06FDat2/P0qVLSU5OxtXVFTc3t9ve/6BBg/D29mbt2rUAfPzx\n",
              "x0ydOhUrK6tq76N79+50796dVatWARAeHs706dPRaDS3HZcQdUEShmg0FEVh+fLlFBcX4+fnh6en\n",
              "J15eXqSnp7NmzRrc3d25ePEi2dnZN3y8Vlu9t8u0adP4+OOPSUxMZNeuXUybNu2m295sn7Nnz+aj\n",
              "jz7i0KFDHDp0iMcee6xazy1EfZKEIRqNrVu3kpCQwJYtWzh8+HDlLSYmBoCYmBhCQkJ49NFHSU9P\n",
              "ByA5OZkTJ04A4OnpicFgoLCw8C+f55FHHuHIkSM8++yzDB48mHbt2t10W09PT+Lj46+7f+LEiZw7\n",
              "d46wsDDGjRuHh4fH7f7aQtQZSRii0Vi2bBl33nknQ4cOxdPTs/LWoUMHwsLCWLZsGd999x1eXl70\n",
              "69cPR0dHRo0aRVpaGgAPPPAA/v7+tGzZEhcXF/bs2XPD5/Hw8GD06NF8//33NzzZfa3nnnuOuLg4\n",
              "XF1dcXFxqbzfzs6ORx99lN9//11OdguToZGZ3kKYpmXLlvH+++8TFxcn5y+ESZAjDCFM0IULF1i8\n",
              "eDHPPvusJAthMiRhCGFi5s6di7e3NwEBAUyfPl3tcISoJCUpIYQQ1SJHGEIIIapFEoYQQohqkYQh\n",
              "hBCiWiRhCCGEqBaTTBiLFy9WOwQhhBB/YpIJIzU1Ve0QhBBC/IlJJgwhhBCmRxKGEEKIapGEIYQQ\n",
              "olosLmEkncvn5YgYSsrK1Q5FCCEsirXaAdSl4tJyJn8cSUZuIc2a2vLiyI5qhySEEBbDoo4wbK21\n",
              "/GO0cf7ysp2n2H/qvMoRCSGE5bCohAEwKsCLib28URSY881hcgqK1Q5JCCEsgsUlDIDXxnShbYum\n",
              "ZOQWMnf9UaQhrxBC1J5FJoymOmsWPxiEtVbDj8cy+SY6Te2QhBDC7FlkwgDo5u3Cc3f5A/D6puOc\n",
              "OpevckRCCGHeLDZhAMwc5Es/3+ZcKSnjma8OU1wqS22FEOJ2WXTC0Go1LHogEBd7G46ezuW9rXFq\n",
              "hySEEGbLohMGgJdzExZO6AbAil1J7E3MVjkiIYQwTxafMABGdvXkoWCfyqW2Fy/LUlshhKipRpEw\n",
              "AP4xujO+bk05m1fESxExstRWCCFqqNEkDHtbaz54sDs2Vhq2HD/L2ihZaiuEEDXRaBIGQNdWzrww\n",
              "wrjU9l/fx5KYdUnliIQQwnw0qoQBEDbAlwHtW1BYUs5Taw9TVFqmdkhCCGEWGl3C0Go1vDcxEFd7\n",
              "G45n5PHuz7LUVgghqqPRJQwADyc73rkvEICPdiezK/6cyhEJIYTpa5QJA2B4Zw+m9NUD8Ny6I5zP\n",
              "L1I5IiGEMG2NNmEA/H1UZ9q7O3Dukiy1FUKIW2nUCaOJrRUfPNgdWyst205ksfq3VLVDEkIIk1Xj\n",
              "hJGQkEBISAh+fn707t2b2NjY67YpLy9nzpw5dO7cmW7dujF06FASExPrJOC61rmlEy/dbRzl+uYP\n",
              "J4g/K0tthRDiRmqcMGbOnMmMGTOIj4/npZdeYurUqddts2nTJvbu3cuRI0eIiYlh2LBhzJs3ry7i\n",
              "rRePhrRhkJ8bRaXlPLX2EIUlstRWCCH+rEYJIysri+joaKZMmQJAaGgoaWlp1x09aDQaioqKKCws\n",
              "RFEU8vLy8Pb2rruo65hWq+Hd+7vRvKktJzMv8fZPJ9UOSQghTI51TTZOS0vDy8sLa2vjwzQaDXq9\n",
              "HoPBQPv27Su3GzNmDL/88guenp44OjrSqlUrdu7cecN9FhUVUVRUdYVSWVnDf8N3d7Tj3/d347HP\n",
              "olm5N4XBfm4M8Xdv8DiEEMJU1ctJ7+joaI4dO8bp06c5c+YMw4YNY9asWTfcdsGCBTg7O1e5RUVF\n",
              "1UdYt3RHRw8e6dcagOfXHeHcJVlqK4QQV9UoYfj4+JCRkUFpaSkAiqJgMBjQ6/VVtvv888+54447\n",
              "cHFxQavV8sgjj/DLL7/ccJ9z584lNze3yi04OPg2f53amzuqE/4ejmTnF/Pit0dkqa0QQlSoUcJw\n",
              "d3enR48erF69GoCIiAi8vb2rlKMAfH192bFjB8XFxrkT33//PV27dr3hPnU6HU5OTlVuVlZWt/O7\n",
              "1Ak7GysWPxSErbWWX+LOsWpfimqxCCGEKalxSSo8PJzw8HD8/PxYuHAhK1euBCAsLIxNmzYB8MQT\n",
              "T9C2bVsCAwPp1q0b27dvZ9myZXUbeT3q6OnEvIqltvN/PMnJzDyVIxJCCPVpFBOsucyZM4dFixap\n",
              "GoOiKDz22QF+iTuHn4cDm54cgJ2Nekc+QgihtkZ9pfdf0Wg0/Pv+QFo46Ig/m8+CzSfUDkkIIVQl\n",
              "CeMvtHDQ8e793QBYtT+VHSfPqhyREEKoRxLGLQzxd+fR/m0AeH5dDFmXCtUNSAghVCIJoxpeGtmR\n",
              "jp6OXLhczPPrYigvN7nTPkIIUe8kYVSDnY0VSx7qjs5ay674c6yUpbZCiEZIEkY1dfBw5JV7OgHw\n",
              "9o8niT2Tq3JEQgjRsCRh1MCUvq25s5M7xWXlPP3VYa4US1dbIUTjIQmjBjQaDW+HdsPNUUdiVj5v\n",
              "bT6udkhCCNFgJGHUUHMHHYsmBgKw+jcDW2IzVY5ICCEahiSM2zCwgxvTB7YF4KWIGM7myVJbIYTl\n",
              "k4Rxm54f4U9nLycuFpTw3DdHZKmtEMLiScK4TTprKz54qDt2Nlr2JGbz8Z4ktUMSQoh6JQmjFtq7\n",
              "O/Dq6C4A/PvnOI6dlqW2QgjLJQmjlh4K9mFEFw9KyhSe+uoQBcWlaockhBD1QhJGLWk0GhZO6IaH\n",
              "k46kc5d543tZaiuEsEySMOqAa1NbFk0MQqOBtVFp/HRMltoKISyPJIw60r99C2YM8gXg5fUxZORe\n",
              "UTkiIYSoW5Iw6tBzw/0JaOVMTkEJc74+QpkstRVCWBBJGHXI1lrL4geDaGJjxf6k86zYJUtthRCW\n",
              "QxJGHfN1c+D1sZ0BeG9LHDHpOeoGJIQQdUQSRj2Y2MuHu7t6Ulqu8PRXh7lcJEtthRDmTxJGPdBo\n",
              "NCyYEICXsx3J2Zf5YHuC2iEJIUStScKoJy72trxxb1cA1kQayL1SonJEQghROzVOGAkJCYSEhODn\n",
              "50fv3r2JjY294XZHjx5lyJAhdOrUiU6dOrF+/fpaB2tuhnVyx9/DkfyiUtZEpqodjhBC1EqNE8bM\n",
              "mTOZMWMG8fHxvPTSS0ydOvW6bQoKCrj33nt58803OXHiBMeOHWPgwIF1Ea9Z0Wg0lddmrNybQmGJ\n",
              "TOgTQpivGiWMrKwsoqOjmTJlCgChoaGkpaWRmJhYZbsvv/ySvn37MmDAAACsrKxwc3Oro5DNy5jA\n",
              "lng523HuUhEbD51WOxwhhLhtNUoYaWlpeHl5YW1tDRi/Qev1egwGQ5Xtjh8/jk6nY/To0QQFBfG3\n",
              "v/2Nc+fO3XCfRUVF5OXlVbmVlVnON3Fbay3TBhiHLa3YlSRzM4QQZqteTnqXlpaybds2wsPDOXTo\n",
              "EK1atWL27Nk33HbBggU4OztXuUVFRdVHWKp5MFiPk501SdmX2XL8rNrhCCFqoaxcYU9CNnGZl9QO\n",
              "pcHVKGH4+PiQkZFBaanxugJFUTAYDOj1+irb6fV6hg4dSqtWrdBoNEyZMoXffvvthvucO3cuubm5\n",
              "VW7BwcG3+euYJgedNQ/3aw3A8p2nUBQ5yhDC3GTmFrJ4WwID397BlE8iGb90L1mNbDxzjRKGu7s7\n",
              "PXr0YPXq1QBERETg7e1N+/btq2w3ceJEDhw4QF5eHgCbN28mMDDwhvvU6XQ4OTlVuVlZWd3O72LS\n",
              "Hglpg621lsNpORxIuah2OEKIaigvV9gVf46ZX0TT/+0dvL8tnjO5xiRRUFzGf39JvMUeLIt1TR8Q\n",
              "Hh7O1KlTmT9/Pk5OTqxcuRKAsLAwxo4dy9ixY9Hr9cybN4+QkBC0Wi2tWrVixYoVdR68OXF3tCO0\n",
              "hzdrowyE7zxFcNtmaockhLiJ8/lFrDuYzpeRBgwXCirv793Glcl9WuNib8PUlQdYG2Vg+kBffJrZ\n",
              "qxhtw9EoJlgfmTNnDosWLVI7jDqXdC6fYYt2oiiw5dlB+Hk4qh2SEKKCoihEJl9gTaSBn45lUFJm\n",
              "/Gh01FkT2tObSX30Vd6zD38Sye6EbEJ7ePPexBtXUCxNjY8wxO3zdXNgRGdPforNJHxnUqP5IxPC\n",
              "lOUWlBDxezprIlM5de5y5f2B3s5M7tOa0YFe2Nte/1H5/F3+7E7IZsOhdGYN9qVDI/gCKAmjgc0c\n",
              "7MtPsZl8d/g0z4/ww8u5idohCdHoKIrC4bQc1kQa+N+RMxSVlgNgb2vFvUEtmRTcmgBv57/cR6CP\n",
              "CyO6ePBz7Fne2xLP8od7NkToqpKE0cC6613p07YZkckX+HRPMn+/p7PaIQnRaOQXlbLx0Gm+jDRw\n",
              "PCOv8v6Ono5M7tuacUEtcbSzqfb+nr/Lny3Hz/JTbCZH0nII9HGph6hNhyQMFcwa3I7I5At8GWng\n",
              "yTs64Nyk+n+gQoiaiz2Ty5pIA98dOs3lYuOFwbbWWkZ382Jyn9b00Lug0WhqvN8OHo6M796K9b+f\n",
              "5t0tcXwxrU9dh25SJGGoYIi/G/4ejsSdvcSayFQeH9L+1g8SQtTIleIyvo85w5pIA4fTcirv923R\n",
              "lEl99NzX0xsXe9taP8+zd/rxvyNn2J2Qzb5T2YS0a1HrfZoqSRgquNqU8Ll1R/h0TwqP9W+LnY3l\n",
              "XXsihBoSsy6xJtJAxMF08gqNFxnbWGkY0cWTyX1a09e32W0dTdyMTzN7HgrW8/n+VN79OY6I2c3r\n",
              "dP+mRBKGSsYGteS9LXGcyS1kw6HTPBSsv/WDhBA3VFRaxk/HMvky0kBk8oXK+71dmzCpj577e/rg\n",
              "5qirt+d/cmh7volO43dDDttPZHFnZ496ey41ScJQiY2VlscGtOXNH07w0a4kJvbywUprmd9KhKgv\n",
              "qecv82WUgXXR6Vy4XAyAVgN3dvJgUh89gzq4oW2A95W7kx1TQ9qyfOcp3t0Sxx0d3RvkeRuaJAwV\n",
              "PRis54PtCSRlX2br8bOM7OqpdkhCmLySsnK2n8hiTWQquxOyK+/3dLLjwWAfHujto8py9VmDfVkT\n",
              "mcrJzEv8L+YM9wa1avAY6pskDBVdbUr44S+nWL7zFCO6eFhs7VOI2jqTc4WvDqTx9QEDZ/OKANBo\n",
              "YFAHNyb30XNHR3esrdSbOu1ib8vMQb68uyWeRVvjGRXghY2K8dQHSRgqmxrSlo92J3M4LYeo5Av0\n",
              "8W2udkgNIr+oFJ211uLeUKJ+HEy9wKSPIisvsGvhYMvEXj48FKw3qT5Oj/Zvy2f7Ukg9X8C66HQm\n",
              "9bGsc5PyblWZm6OO+3p6AxC+K0nlaBpG2oUChvz7Fwa8vYOj6blqhyNM3JXiMp5fF0NRaTmB3s78\n",
              "d1J39r08jBdHdjSpZAHQVGfNE0ONy+Q/2J5gcWOZJWGYgOkDfdFoYMfJLIsfyqIoCvM2HCU7v5iz\n",
              "eUVMDN/PNhkqJf7Cu1viSM6+jKeTHZ9P68Pobi2xtTbdj65JffS0dLYjM6+QL/anqh1OnTLdV70R\n",
              "aduiKSO7GE94r7Dwo4xvD6azOyEbW2stwW2bcaWkjBlfRLNqX4raoQkTFJ1ygU/3JgOwIDTALLoi\n",
              "6KyteOZOPwCW/prIpcISlSOqO5IwTMSMQb4AfHf4NBm5V1SOpn5k5RXyxvfHAePVsWvC+vBgbx/K\n",
              "FXhtUyz/+t9xymTmuahQWFLGC9/GoChwf09vhvq7qx1StU3o0Qpft6ZcLCjhkz3JaodTZyRhmIir\n",
              "TQlLyxU+2W05f2DXevW7WPIKS+nayonpA9tiY6VlwYQAXhjhD8Cne5OZtfogBcWlKkcqTMG7PxtL\n",
              "UR5OOl4ZbV5NOq2ttDw33Ph3/fHu5MprRMydJAwTMmtIOwDWRhnILbCcw1iAzUcz+Ck2E2uthndC\n",
              "AyuXP2o0Gp4Y2p4lD3XH1lrL1uNneXDFb2RdalyzkkVVB1Mv8ElFKWrhhG5mUYr6s7u7etKlpRP5\n",
              "RaUs+9UyRrlKwjAhQ/yMTQkvF5exOtJyTpblFBTz6nfHAJg9pB2dWzpdt82YwJZ8GdYHV3sbYtJz\n",
              "Gf/hPuLPWvYCAHFjhSVlvLDOWIq6r6c3QzuaTynqWlqtpvLoedX+VIsoNUvCMCEajYaZg43nMlbu\n",
              "TbGYJXn/+v442fnFtHd34Mk7bt6Zt1ebZmx4vD9tWzTldM4VQpftY29i9k23F5bpvS1xJFWUov5h\n",
              "ZqWoPxvs50Zwm2YUl5bzwXbzP8qQhGFixgS2pKWzHdn5RWw4dFrtcGrt17gs1v9+Go0G3g7ths76\n",
              "r7vytmnRlPWzQ+jdxpVLhaU88mkU66LTGihaobaDqRf4uOIk8YIJ5rEq6q9oNBpeGGk8yvgmOo2U\n",
              "7Mu3eIRpk4RhYq42JQTjEltzXjWUX1TK3zcYS1FTQ9rQs7VrtR7n2tSWL6b1YWxgS0rLFV74Nob3\n",
              "tsShKOb7Wohbu7YUFdrDmzs6WkbH195tmjHU342ycoX3t8WrHU6tSMIwQQ8F63FuYkNy9mW2Hs9U\n",
              "O5zb9s5PJzmdcwVv1yaVtdzqsrOx4j8PBPFkxVWzS3Yk8szXhykqtYwynbjeoq3xlaWoV828FPVn\n",
              "z91l/PvfdOQMJ64ZDWtuapwwEhISCAkJwc/Pj969exMbG3vTbRVF4Y477sDFxaU2MTY6TXXWPNy3\n",
              "NQDLdiaZ5TfrqOQLfF5xlevCCd2wt6152zKtVsPzI/x5J7Qb1loN3x0+w8MfR3HRQpYoij8cTL3I\n",
              "R7uNF60umBCAs715l6L+rGsrZ+7p5oWiGM/RmKsaJ4yZM2cyY8YM4uPjeemll5g6depNt33//fdp\n",
              "165dbeJrtB4JaYOttZYjFU0JzUlhSRkvR8QA8EAvHwZ0qN3Iyom9ffjs0WAcddZEpVxgwrJ9Zl8L\n",
              "Fn8wXqB3BEUxXvBmKaWoP3tuuB9WWg3bTmRxMPWi2uHclholjKysLKKjo5kyZQoAoaGhpKWlkZh4\n",
              "/dn/2NhYNm7cyMsvv1w3kTYy5tyUcHHFjA93Rx3z7ulUJ/sc0KEF384OoZVLE5KzLzNh2T4OpppX\n",
              "IhU3tmhrPEnnjH8vr43uonY49cbXzYH7ehjf0//++aRZVg5qlDDS0tLw8vLC2tpYXtBoNOj1egwG\n",
              "Q5XtSkpKmD59OuHh4VhZ/fWqmKKiIvLy8qrcysqkTg3m2ZTw2Oncyn5Yb47rWqerXPw9HdnweAgB\n",
              "rZy5cLmYhz6K5PuYM3W2f9HwDqZe5GMLLkX92VN3dsDWSstvSRfYY4ZLxuvlpPc///lPJkyYQKdO\n",
              "t/52uWDBApydnavcoqKi6iMss9O2RVPurpjCF77rlMrR3FpJWTkvfBtDWbnCPd28uKtL3U8QdHey\n",
              "4+uZfbmzkwfFpeU8+eUhlv16yiy/rTV2V0tR5RWlqGGdLLMUda1WLk2YUnF+8t8/m9/KvxolDB8f\n",
              "HzIyMigtNfb6URQFg8GAXl91SMjOnTtZsmQJbdq0YcCAAeTl5dGmTRvOnTt33T7nzp1Lbm5ulVtw\n",
              "cHAtfiXLMnOQ8RzQpsNnOJNj2leKhu88xYmMPFzsbfjn2PorLdjbWhP+cE8e7d8GgLd/Osm8DUcp\n",
              "KSuvt+cUde/9RlKK+rPHh7bD3taKmPRcfo41r1WQNUoY7u7u9OjRg9WrVwMQERGBt7c37dtXvXp3\n",
              "9+7dpKamkpKSwp49e3ByciIlJQU3N7fr9qnT6XBycqpyu1UZqzEJ9HGhr6+xKeGnJtz1MjHrUuWV\n",
              "rK+N6UwLB129Pp+VVsNrY7rw2pjOaDWwNiqNxz47YFGtpC3Z74Y/VkXNH2/5pahrtXDQMa3iWqt3\n",
              "t8Sb1bVWNS5JhYeHEx4ejp+fHwsXLmTlypUAhIWFsWnTpjoPUMDMwabdlLCsXOHFb2MoLitnqL8b\n",
              "44JaNdhzP9q/LeEP96KJjRW7E7K5f/l+kz8Sa+yMF+hVlKK6t+LOzpZfivqz6YN8cW5iQ2JWPhvN\n",
              "qKNDjROGv78/+/fvJz4+nujoaAICAgD4+OOPGTt27HXbt2nThpycnFoH2pgN8XOjo6fpNiX8fH8K\n",
              "vxtycNBZ89b4ADQaTYM+//DOHnwzsx9ujjpOZl5i3Id7OXZaRr+aqve3xXPq3GXcHHW8OsayLtCr\n",
              "Lic7G2ZXdKd+f1s8xaXmUU6VK73NQNWmhMkm1ZQw7UIB7/xkvBDp5bs70tKliSpxBHg7s+HxEPw8\n",
              "HMi6ZBz9uv2EjH41NYcMF/moYhXdgvEBuNjbqhyReh7p1wZ3Rx3pF6/w1QHDrR9gAiRhmInR3a42\n",
              "JSxm/e+mcQirKApz1x/lSkkZwW2bMSlYf+sH1SNvV3u+nR3CgPYtKCguY/rn0Xy+P0XVmMQfCkvK\n",
              "eL6iFDW+kZairtXE1or/G9YBgA+2J5rF4DBJGGbCxkrLtIHGo4yPdptGU8J10ensScxGZ63l7dBu\n",
              "aLUNW4q6ESc7G1Y+2psHehlHv776XSxvfC+jX03Bf7YlVJaiXmukpag/e6CXDz7NmpCdX8RnZjDX\n",
              "XhKGGXmwt4/JNCU8m1fIGz8Y53PPGe5H2xZNVY3nWjZWWhaG/jH69ZM9yTy+5iBXik2nlNfYHDJc\n",
              "ZEXFtUTzG3kp6lq21lrmDPcDYPmvp8i9YnqLWq4lCcOMNNVZ87d+6jclVBSFf2w8xqXCUrp5O1cu\n",
              "ETQlV0e/fvBQd2yttPwce5YHV+zn3KUitUNrdIwX6MVQrsC4oJYMb+SlqD8bG9gKPw8H8gpLK8/v\n",
              "mCpJGGbm2qaEkSo1Jdx8NJMtx89irdXwdmi3yvncpmhsYEvWTO+Di70NR9JzGffhXhJk9GuD+s+2\n",
              "BBKz8mnhoOP1eryg01xZaTWV7c8/3Zts0l9qTPedLm6ohYOO+682JdzZ8O1CLl4u5rVNxqFIjw9t\n",
              "Tyev6+dzm5reFaNf2zS353TOFSYs28c+M+zjY44Op+VcU4rqKqWom7irsweBPi4UFJfx4S+mO8pV\n",
              "EoYZutqU8Je4c5zMbNhhLG9UzOfu4O7AE0PNp3V92xZNWf94f3q1No5+/dunUXx7MF3tsABjiS+v\n",
              "sATD+QJi0nM4kZFndj2GbuTaC/TGBbWsl95ilkKj0fBixTm3LyMNpF8sUDmiG6v5VBuhujYVTQk3\n",
              "H81kxa4kFk0MapDn/eVkFusPGedzv3Pfredzm5pmTW1ZHdaHF76N4X9HzvD8uiMYzl/m2eF+dXKx\n",
              "YXm5wqXCUi4WFJNzpYScgmJyr5SQU1Bxu1Jc8f/Gf88tKDH+90rJdau4hnf2YP74ANwc67fFSn1a\n",
              "vD2BhIpS1GtjpBR1K/3btyCkXXP2nTrP4m0J/Pv+QLVDuo4kDDM1c1A7Nh/NZNPhMzx3lz+t6vmC\n",
              "uUuFJfx9w1EAHuvflu766s3nNjV2NlYsfiAIfbMmfPjLKT7YkYjhQgFvX5MAy8oV8q6UVH7w51b5\n",
              "sDd+wF8sqPj5Sgm5V7e7UkJtDgya2FjhYm9Ddn4RW4+f5WDqRd4a15W7A7zq6LdvOEfScipLpvPH\n",
              "d8W1qZSiquOFEf6MX7qPiN/TmTm4He3dHdQOqQpJGGYq0MeFfr7N2Z90nk/3JPOPep6B/M5PcZzJ\n",
              "LUTfzJ7n7vKr1+eqb1qthhdGdETfzJ55G46x8fAZDqRcxEqrIaegmLzC2l1A1dTWChd7W5yb2OBi\n",
              "b4OrvS3O9ja4VPzs0sT4s6u9bcXPNjg1scHOxpiwTmTk8ezXhzmZeYnZa35nXFBL/jm2q9k06Lv2\n",
              "Ar17pRRVI931rgzv7MHW42dZtDWOpZN7qh1SFZIwzNjMwb7sTzrP2igDT93Rod4+UCKTzvPFb1fn\n",
              "cwfc1nxuU/RAbz0tXZrw+OrfOX2DhoWOOmvjB/01H/IuTf74oDcmhD8+9K8mCVvr2p0a7OTlxKYn\n",
              "B7B4ezzLfj3FxsNn2J90nnfuC2Sw3/Udn03NB9eUol6XUlSNPX+XP9tOnGXz0UyOpucS4O2sdkiV\n",
              "LOOd30gNrmhKeDLzEqsjU3liaPtbP6iGCkvKeHm9sRT1ULAPIe1rN5/b1Azs4Mb25wdz7HQuzk1s\n",
              "cG7yRzKwUXG5sK21lhdGdGRYJw+e/+YISdmXeeTTKCb10fP3UZ1oqjPNt+6RtByWV5Si3pJS1G3x\n",
              "93RkXFArNhw6zbtb4lj1mOnMB5JVUmasIZoSvr8tnuTsy3g46Zg7qm7mc5sad0c77ujoQc/WzWjv\n",
              "7kALB52qyeJaPfSu/PDUQKaGtAGMK2juXrybKJWuwfkrRaV/TNAbG9iSEVKKum3P3NkBa62GnfHn\n",
              "iEw6r3Y4lUzjXSFu2+huLWnl0oTs/GIifq/bZaIx6TmVV56+NS4AJzvzqKFbmia2Vrw+tgtfhvWh\n",
              "lUsTDBcKeGDFft764bhJdS7+YHsC8WfzaeFgW68TFxuD1s2b8kBvH8C0RrlKwjBzNlbaytYcH+2q\n",
              "u6aExaXlvFjRzmFMYMtG31nUFIS0b8GPzwxkYi9vFAU+2p3M6CV7iEnPUTs0YtJzWL7T+OXizXEB\n",
              "UoqqA08N64DOWkt06kV+jbt+vLUaJGFYgAcqmhKmnC9gSx3NCA7feYqTmZdwtbfhdeksajKc7Gx4\n",
              "575APv5bL1o46EjMymf80n28vzVetZnmRaXGVVFl5QpjAlsysquUouqCh5NdZSnynZ/jKDeBjsuS\n",
              "MCzAtU0Jl+88VevD14Szl1iyw9ie4PWxXWhez/O5Rc3d2dmDrc8O4p5uXpSVKyzensCEpftU6ZMl\n",
              "paj6M2twOxx11pzIyOOHoxlqhyMJw1I8EtIGnbWWI+m5tWpKWFau8GKEcT73sI7ujA1sWYdRirrk\n",
              "2tSWDyf14IOHuuPcxIajp3O5Z8keVuw61WDzP6qWorrSTEpRdcq1qS3TBxkXtizaGk+pSkeRV0nC\n",
              "sBAtHHTc38vYlHB5LZoSfrYvhUOGHBx11rw5vmuDz+cWNTc2sCVbnh3EEH83ikvLmb/5JA+t+A3D\n",
              "+frtR1RUWsYL62KuKUWZ3xXp5uCxAW1p1tSW5OzLqvc/k4RhQcIG+KLVwK9x5ziRUfOmhIbzBbz7\n",
              "s3E+99xRnfByVmc+t6g5Dyc7Vk7tzcIJATS1tSIq5QIjF+9iTWRqva2wWbI9kbizl6QUVc8cdNY8\n",
              "PsTY6HPx9gRVV8ZJwrAgxqaExm95NR3EoigKczfEcKWkjL6+zXiwYkmfMB8ajYYHg/X89Mwg+rRt\n",
              "RkFxGX/fcIxHVh4gM7ewTp/raHouyyqOZKUUVf+m9G2Nl7MdGbmFrIk0qBZHjRNGQkICISEh+Pn5\n",
              "0bt3b2JjY6/bZseOHQQHB9O5c2e6dOnCiy++SHm5urW3xmJGRb1z05EzN2x3cTPfRKexN/E8djZa\n",
              "Fk4wjfnc4vb4NLNn7fS+vHJPJ2ytteyKP8dd7+9k46HTdXK0ce2qqNHdvKQU1QDsbKx4elgHAD78\n",
              "JZH8otr1O7tdNU4YM2fOZMaMGcTHx/PSSy8xderU67ZxdXXlq6++4vjx4xw8eJB9+/bx+eef10W8\n",
              "4hauNiUsLVf4dE9ytR5zNq+QN384AcBzw/1pY0LzucXt0Wo1hA30ZfNTAwj0diavsJRnvj7M42t+\n",
              "53x+7Sa6/XeHsRTVvKmUohrSfT29aduiKRcuF1f7vV3XapQwsrKyiI6OZsqUKQCEhoaSlpZGYmLV\n",
              "CVHdu3fH19f4TdfOzo6goCBSUlLqJmJxS7Mq6p1rowzkFBT/5baKovD3Dcb53IE+LjxmgvO5xe1r\n",
              "7+5IxOwQnhvuh7VWw4/HMhnxn123fb3OsdO5LP31j1KULLluONZWWuYMN3aK/mhXEhcv//V7uz7U\n",
              "KGGkpaXh5eWFtbWx8ZlGo0Gv12Mw3LymlpmZybfffsvo0aNv+O9FRUXk5eVVuZWVmU67A3M0qEML\n",
              "Ono6UlBcxuqKLrM3831MBttOnMXGSsM7od2wklKUxbG20vJ/wzqw8Yn++Hs4kp1fzIwvDjLnm8Pk\n",
              "Ximp9n6KS8srS1H3dPMyyzkd5u6eAC86eTlxqai0Vqshb1e9nvTOy8tjzJgxvPjii/Tq1euG2yxY\n",
              "sABnZ+cqt6ioqPoMy+JpNBpmDTYeZXy2L+WmqyouXC7m9U3Gc1BPDG2Pv6djg8UoGl7XVs5s+r/+\n",
              "zBrcDq0G1v9+mpH/2cWehOrNN//vjgROZhpLUf+SUpQqjLNcjEcZn+1L4Wxe3S5muOXz12RjHx8f\n",
              "MjIyKC01nnBRFAWDwYBer79u20uXLjFy5Ejuvfde5syZc9N9zp07l9zc3Cq34GDTaedrru7p5nXL\n",
              "poT/+l8s5y8X4+/hyOND6r41ujA9OmsrXr67I+tm9aN1c3sycguZ8kkkr353jILim59IPXY6lw8r\n",
              "SlFvSClKVUP93enV2pWi0nKW7Eho0OeuUcJwd3enR48erF69GoCIiAi8vb1p377qh01+fj4jR45k\n",
              "5MiRvPLKK3+5T51Oh5OTU5WblZV5zYo2RbdqSrjj5Fk2Hj6DVgNv39et1kN/hHnp2boZPz49kIf7\n",
              "GlvKfL4/lVGLd3Mw9fouAVVKUQFejJJSlKo0Gg0vjPAH4KuoNFLPX26w567xp0R4eDjh4eH4+fmx\n",
              "cOFCVq5cCUBYWBibNm0CYPHixURFRbF+/XqCgoIICgrirbfeqtvIxS09GOyDi72xKeHP15zkNM7n\n",
              "PgbAtAFtCfJxUSlCoSZ7W2veGNeVL6YF4+VsR8r5Au5fvp+FP56kqPSPMubVUlSzprb8614pRZmC\n",
              "Pr7NGeTnRmm5wn+2NdxRhkYxlUbr15gzZw6LFi1SOwyLsGhLHB/sSCTQ25mNT/RHo9Hw9w1HWRNp\n",
              "oHVze356ehBNbOWIrrHLvVLCv/53vLJ86e/hyKIHAlEUuPfDvZSVK3w4qQf3dJOjC1NxND2XMf/d\n",
              "g0YDPz09qEHOQUodwsL97ZqmhL8lXeC3pPOVV4oumBAgyUIA4NzEhvcmBhL+cE9aONgSd/YS9/53\n",
              "L2GroitLUZIsTEuAtzOjAjxRFHh3S1yDPKckDAt3bVPCJTsSeDkiBoCHgvWEtLOs+dyi9kZ08eTn\n",
              "ZwYxsosnpeUKmXmFNGtqyz+lFGWS5gz3Q6uBrcfPcshwsd6fzzQnyYs6NX2gL19GGth3yjgb2NPJ\n",
              "jrmjOqoclTBVzR10LJvSg42HT7M2Ko2n7uhAC1kVZZLauzsS2sObzLxC7G3r/+NcEkYj0Lp5U+4O\n",
              "8OKHGOMAlrfGd5X53OIvaTQaxnf3Znx3b7VDEbfw1viABlvlKCWpRuKpOzrgZGfNw31bM6yTzOcW\n",
              "wlI05JJ4OcJoJPw9HTny2l1qhyGEMGOSMBoRmZ4nhKgNKUkJIYSoFkkYQgghqkUShhBCiGoxydYg\n",
              "EyZMoE2bNrf12LKyMqKioggODpYmhn8ir82Nyetyc/La3JilvS6tW7fm6aefvuV2JpkwaiMvLw9n\n",
              "Z2dyc3NxcnJSOxyTIq/NjcnrcnPy2txYY31dpCQlhBCiWiRhCCGEqBZJGEIIIarF4hKGTqfjtdde\n",
              "Q6eTZml/Jq/NjcnrcnPy2txYY31dLO6ktxBCiPphcUcYQggh6ockDCGEENUiCUMIIUS1WFzCSEhI\n",
              "ICQkBD8/P3r37k1sbKzaIamusLCQcePG4efnR2BgIMOHDycxMVHtsEzKypUr0Wg0bNy4Ue1QTEZR\n",
              "URFPPvkkHTp0ICAggClTpqgdkknYvHkzPXr0ICgoiK5du7Jq1Sq1Q2o4ioUZOnSosnLlSkVRFGXd\n",
              "unVKr1691A3IBFy5ckX54YcflPLyckVRFGXJkiXK4MGD1Q3KhCQnJyv9+vVT+vbtq2zYsEHtcEzG\n",
              "M888ozz55JOVfzcZGRkqR6S+8vJyxdXVVTly5IiiKMa/HZ1Op+Tl5akcWcOwqCOMrKwsoqOjK78J\n",
              "hYaGkpaW1ui/TdvZ2TFq1KjKeRh9+/YlJSVF3aBMRHl5OWFhYSxZsqTRLZH8K5cvX+aTTz7hrbfe\n",
              "qvy78fT0VDkq06DRaMjJyQGMLUKaN2/eaP52LCphpKWl4eXlhbW1cS6URqNBr9djMBhUjsy0LF68\n",
              "mHvvvVftMEzCokWL6N+/Pz179lQ7FJNy6tQpmjVrxvz58+nVqxcDBw5k+/btaoelOo1Gw9dff82E\n",
              "CRNo3bo1AwYMYNWqVdja2qodWoOQiXuNzPz580lMTJQ3P3Ds2DEiIiLYtWuX2qGYnNLSUlJTU+nc\n",
              "uTMLFy7k0KFDDB8+nNjYWDw8Gu9M+NLSUt58803Wr1/PoEGDOHDgAGPHjuXo0aO0aNFC7fDqnUUd\n",
              "Yfj4+JCRkUFpaSkAiqJgMBjQ6/UqR2Ya3n33XdavX8+PP/6Ivb292uGobvfu3aSkpNChQwfatGnD\n",
              "b7/9xowZM1i2bJnaoalOr9ej1WqZPHkyAN27d6dt27YcPXpU5cjUdfjwYc6cOcOgQYMA6N27N97e\n",
              "3hw6dEjlyBqI2idR6trgwYOrnPTu2bOnugGZiPfee0/p0aOHcuHCBbVDMVmDBw+Wk97XGD58uPLD\n",
              "Dz8oiqIoSUlJSvPmzZX09HSVo1JXZmam4uDgoBw/flxRFEVJSEhQXF1dldTUVJUjaxgW1xokLi6O\n",
              "qVOncv78eZycnFi5ciUBAQFqh6Wq9PR0fHx88PX1xdHRETD2womMjFQ5MtMyZMgQnnnmGcaNG6d2\n",
              "KCYhKSmJadOmkZ2djVar5dVXXyU0NFTtsFS3du1a5s+fj1arpby8nLlz5zJp0iS1w2oQFpcwhBBC\n",
              "1A+LOochhBCi/kjCEEIIUS2SMIQQQlSLJAwhhBDVIglDCCFEtUjCEEIIUS2SMIQQQlSLJAwhhBDV\n",
              "IglDCCFEtUjCEEIIUS2SMIQQQlTL/wPBLyPyhu4qjAAAAABJRU5ErkJggg==\n",
              "\">\n",
              "        \n",
              "      </div>\n",
              "      <script></script>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-795a2fee-9beb-497d-aacb-30a117f4312c\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-795a2fee-9beb-497d-aacb-30a117f4312c\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x7caa8435c850>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "import numpy as np\n",
              "from google.colab import autoviz\n",
              "\n",
              "def histogram(df, colname, num_bins=20, figscale=1):\n",
              "  from matplotlib import pyplot as plt\n",
              "  df[colname].plot(kind='hist', bins=num_bins, title=colname, figsize=(8*figscale, 4*figscale))\n",
              "  plt.gca().spines[['top', 'right',]].set_visible(False)\n",
              "  plt.tight_layout()\n",
              "  return autoviz.MplChart.from_current_mpl_state()\n",
              "\n",
              "chart = histogram(_df_1, *['Activity'], **{})\n",
              "chart"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-00441650-3e66-4861-b655-9e61a3e17276\">\n",
              "        <img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYwAAADECAYAAACMRRb/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAVEklEQVR4nO3deXBV1QHH8d9LsAGahEBACCAosoRESADNBAQhtsgiQqRoR6wC\n",
              "EhIWSwu0VKwd1gK1VsV2AgEppcVhB3ECrogWWkS2ApGyRAiLNQYF80CSAMnpH45viAQ9N+QmF/L9\n",
              "zLyZvPvu8svLwG/uPe+e5zPGGAEA8D2CqjoAAOD6QGEAAKxQGAAAKxQGAMAKhQEAsEJhAACsUBgA\n",
              "ACsUBgDACoUBALBCYQDX4Pjx4woNDdWRI0es1p85c6buu+8+l1MB7qAwUC0tWrRIPp9PTz31lPU2\n",
              "U6ZMUdeuXUsta9asmc6dO6cWLVpY7ePpp5/WW2+9FXjeo0cPPfPMM9YZgKpEYaBaSk9PV2RkpP76\n",
              "17+qqKioquMA1wUKA9XO9u3btWPHDi1ZskT5+flauXJl4LXTp09r9OjRuu222xQWFqbo6Gi9+eab\n",
              "euWVVzRz5kxt3bpVoaGhCg0N1ebNm5WTkyOfz6fs7Gzl5+erdu3a2rx5c6njjR07Vv3795dU+ixl\n",
              "5MiR2rx5s5599tnAPv1+v0JDQ/X++++X2seTTz6p5ORkd98Y4HtQGKh20tPTFR8fr969e+vBBx9U\n",
              "enq6JMkYo+TkZOXk5Oj999+X3+/Xhg0bdMstt+jRRx/V008/rc6dO+vcuXM6d+6cunXrVmq/derU\n",
              "0aBBg7Rw4cLAssLCQi1ZskQpKSlX5Jg3b566deumiRMnBvYZHh6uwYMHa/78+YH1zp8/ryVLlmjk\n",
              "yJEuvSOAHQoD1cqZM2e0fPlypaamSpJSU1O1detW7dmzRzt37tSWLVu0ePFiNWvWTD6fTy1atFBM\n",
              "TIz1/lNSUrRy5Ur5/X5J0urVq1WzZk3df//91vsYNWqUVq9erS+++EKStGzZMtWrV0+9evVy8JsC\n",
              "FY/CQLXyzWD3o48+KklKSkpSy5YtlZ6erqNHj6pu3bpq0KBBufd/zz33qGnTplq6dKkk6eWXX9bQ\n",
              "oUMVHBxsvY8OHTqoQ4cOWrx4sSQpIyNDI0aMkM/nK3cuoCJQGKg2jDGaN2+eLly4oNatW6tRo0aK\n",
              "iorSyZMn9corr+jmm2/WmTNn9Pnnn5e5fVCQ3T+X4cOH6+WXX1Z2drb++c9/avjw4Vdd92r7HDVq\n",
              "lBYsWKDdu3dr9+7deuKJJ6yODbiJwkC18fbbb+vw4cN666239J///Cfw2Lt3ryRp79696tKli4YN\n",
              "G6aTJ09Kko4ePar//ve/kqRGjRrp+PHjKiws/M7jDBkyRHv27NG4cePUvXt33X777Vddt1GjRjp0\n",
              "6NAVyx9++GGdOnVKKSkpSk5OVsOGDcv7awMVhsJAtTF37lz9+Mc/VlJSkho1ahR4tGrVSikpKZo7\n",
              "d67WrVunqKgode7cWWFhYerbt69OnDghSfrpT3+qNm3aqHHjxoqIiNCWLVvKPE7Dhg3Vr18/ZWZm\n",
              "ljnYfbkJEybo4MGDqlu3riIiIgLLa9asqWHDhmnXrl0MdsMzfHynN+BNc+fO1QsvvKCDBw8yfgFP\n",
              "4AwD8KDTp09rzpw5GjduHGUBz6AwAI+ZNGmSmjZtqnbt2mnEiBFVHQcI4JIUAMAKZxgAACsUBgDA\n",
              "CoUBALBCYQAArHi2MObMmVPVEQAAl/FsYRw7dqyqIwAALuPZwgAAeEsNN3d+3333KTc3V0FBQQoL\n",
              "C9NLL72kDh06uHlIAIBLXC2MFStWBCZUW7t2rYYOHao9e/a4eUgAgEtcvSR1+eyb+fn5zIkDANcx\n",
              "V88wJOnxxx/Xpk2bJEkbNmwoc52ioiIVFRWVWlZcXOx2NACAA5U2l9TixYu1fPnyMktjypQpmjp1\n",
              "aqlliYmJ2rp1a7mPd+tT68u9rSTlzLb/DmYAqA4q7VNSQ4YM0aZNmwJfbH+5SZMmKT8/v9QjISGh\n",
              "sqIBACy4dknqyy+/1Pnz59W4cWNJ0quvvqrIyEjVq1fvinVDQkIUEhJSallwcLBb0QAA5eBaYeTn\n",
              "5+uhhx5SQUGBgoKC1KBBA2VmZjLwDQDXKdcKo3nz5vrwww/d2j0AoJJxpzcAwAqFAQCwQmEAAKxQ\n",
              "GAAAKxQGAMAKhQEAsEJhAACsUBgAACsUBgDACoUBALBCYQAArFAYAAArFAYAwAqFAQCwQmEAAKxQ\n",
              "GAAAKxQGAMAKhQEAsEJhAACsUBgAACsUBgDACoUBALBCYQAArFAYAAArFAYAwAqFAQCwQmEAAKxQ\n",
              "GAAAKxQGAMAKhQEAsEJhAACsOC6MN998040cAACPc1wY06ZNU5s2bTRnzhz5/X43MgEAPMhxYfzr\n",
              "X//SsmXLlJWVpdatW2v06NHav3+/G9kAAB5SrjGMDh06aMGCBXrjjTeUmZmp9u3bq2fPntq3b19F\n",
              "5wMAeES5CuOdd97RgAEDNHDgQI0ZM0a5ublKS0vTgw8+GFinsLBQycnJat26teLi4tSzZ09lZ2dX\n",
              "WHAAQOWq4XSDtm3bqn79+ho7dqwGDhyo4OBgSdKgQYO0cOHCUuumpqaqT58+8vl8+stf/qKUlBS9\n",
              "9957FRIcAFC5HBfGkiVL1KlTpzJfe/311wM/16xZU3379g08T0xM1HPPPVeOiAAAL3B8SWrnzp06\n",
              "ffp04PkXX3yhBQsWfO92c+bM0YABA8p8raioSH6/v9SjuLjYaTQAgIscn2Gkp6crNTU18DwyMlLp\n",
              "6ekaMWLEVbeZOXOmsrOztXHjxjJfnzVrlqZOnVpqWWJiotNogCTp1qfWl3vbnNn3V2AS4Mbi+AzD\n",
              "GHPFsu86G3juuee0Zs0avf7666pdu3aZ60yaNEn5+fmlHgkJCU6jAQBc5LgwoqKitGLFisDz5cuX\n",
              "Kyoqqsx1n3/+eS1dulRvv/22IiIirrrPkJAQhYeHl3p8M5gOAPAGx5ekXnzxRQ0YMEATJ06UJNWu\n",
              "XVvr1q27Yr2TJ09qwoQJatGihZKSkiR9XQzbtm27xsgAgKrguDCio6O1f/9+HTx4UJLUpk2bMs8G\n",
              "mjZtWublKwDA9clxYUiSz+dTRESELl26pE8++USS1KxZswoNBgDwFseF8be//U1jx47VTTfdpKCg\n",
              "r4dAfD6f8vLyKjwcAMA7HBfG9OnTtX37drVp08aNPAAAj3L8Kan69etTFgBQDTkujOTkZL344ovK\n",
              "y8srdWc2AODG5viS1G9/+1tJ0vjx4+Xz+WSMkc/nYyoPALjBOS6MkpISN3IAADyuXN+HsXPnTv3j\n",
              "H/+QJH355Zf69NNPKzQUAMB7HBdGenq6nnjiCU2ZMkXS17PVDh48uKJzAQA8xnFhzJ8/Xx988IHC\n",
              "w8MlSbfffrtOnTpV4cEAAN7iuDBCQkJUq1atUstq1CjXDeMAgOuI48Jo0KCBDh06JJ/PJ+nrO7+Z\n",
              "FgQAbnzlmq32kUce0YEDB3TLLbcoPDxcmZmZbmQDAHiI48Jo2bKltm3bpoMHD8oYc9XZagEANxbH\n",
              "hXH8+HFJ0g9/+ENJYrZaAKgmHBdGp06dAnd4FxYW6vz584qMjGS2WgC4wTkujG9/hHbNmjXas2dP\n",
              "hQUCAHhTue70vtzAgQO1fv36isgCAPAwx2cYl89MW1xcrG3btjFbLQBUA44LIyIiIjCGERwcrFat\n",
              "Wumll15yIxsAwEOYrRYAYOWaxzAAANWD4zOMoKCgwLQgl+OLlADgxua4MKZNm6aCggKNGjVKkjRv\n",
              "3jzVqlVLv/zlLys6GwDAQxwXxtq1a7Vz587A8xkzZqhTp06Br24FANyYHI9hnD17ttRd3Xl5eTp7\n",
              "9myFhgIAeI/jM4wJEyYoLi5Offv2lSS98cYbgW/fAwDcuBwXRlpamu6++25t2rRJkjR+/HjFxsZW\n",
              "eDAAgLeU66vyIiMj1a5dO/Xo0UOXLl3ShQsX9IMf/KCiswEAPMTxGMaqVauUmJioYcOGSZI++ugj\n",
              "JScnV3QuAIDHOC6MWbNmadeuXYqIiJAkxcXF6dixYxWdCwDgMY4LIzg4WJGRkaWWcTkKAG58jgsj\n",
              "LCxMn332WeBu740bN6pevXoVHgwA4C2OB73/8Ic/qE+fPjpy5Ii6du2qo0eP8n0YAFANOCqMkpIS\n",
              "FRcXa9OmTfr3v/8tY4y6dOkSGM/4trFjx+q1117TsWPHtHv3bsXHx1dAZABAVXB0SSooKEipqamq\n",
              "U6eO+vTpo759+161LCRp0KBB2rJli5o3b36tOQEAVczxGEarVq2UnZ1tte4999yjpk2bOg4FAPAe\n",
              "x2MYp0+fVnx8vLp06aLQ0NDA8jVr1pQ7RFFRkYqKikotY5p0APAW68JITU3V/PnzNWTIEPXv3191\n",
              "69atsBCzZs3S1KlTSy1LTEyssP1fb259qmo/RJAz+/4qPX51xt++errWv3tl/d2sC2PHjh2SpCFD\n",
              "hqhjx47atWtXhYWYNGmSxo8fX2rZ7373uwrbPwDg2pVrLiljTIWGCAkJUUhISKllwcHBFXoMAMC1\n",
              "sS6MgoIC7du3T8YYFRYWBn7+Rvv27a/YJi0tTevXr1dubq569eqlsLAw6wFzAIC3OCqM/v37B55f\n",
              "/rPP59ORI0eu2CYjI+Ma4wEAvMK6MHJyclyMAQDwOsf3YQAAqicKAwBghcIAAFihMAAAVigMAIAV\n",
              "CgMAYIXCAABYoTAAAFYoDACAFQoDAGCFwgAAWKEwAABWKAwAgBUKAwBghcIAAFihMAAAVigMAIAV\n",
              "CgMAYIXCAABYoTAAAFYoDACAFQoDAGCFwgAAWKEwAABWKAwAgBUKAwBghcIAAFihMAAAVigMAIAV\n",
              "CgMAYIXCAABYoTAAAFYoDACAFQoDAGDF1cI4fPiwunTpotatW+uuu+7SRx995ObhAAAucrUw0tLS\n",
              "lJqaqkOHDuk3v/mNhg4d6ubhAAAucq0w8vLytGPHDv3sZz+TJP3kJz/RiRMnlJ2d7dYhAQAuquHW\n",
              "jk+cOKGoqCjVqPH1IXw+n5o1a6bjx4+rZcuWpdYtKipSUVFRqWXFxcVuRQMAlIPPGGPc2PHOnTs1\n",
              "ePBgHTx4MLAsISFBs2fP1r333ltq3SlTpmjq1KmllkVHR6tPnz5uRLsmxcXF+vDDD5WQkKDg4OCq\n",
              "jnMFL+cjW/mQrfy8nM9L2Zo3b65f/OIX37uea4WRl5enli1b6vTp06pRo4aMMYqKitKWLVuszjBC\n",
              "QkIUEhLiRrRr4vf7VadOHeXn5ys8PLyq41zBy/nIVj5kKz8v5/NytqtxbQzj5ptvVseOHbVkyRJJ\n",
              "0urVq9W0adMrykL6uhzCw8NLPbxYFgBQnbk2hiFJGRkZGjp0qGbOnKnw8HAtWrTIzcMBAFzkamG0\n",
              "adNGW7dudfMQAIBKwp3eDoWEhGjy5MmevWTm5XxkKx+ylZ+X83k529W4NugNALixcIYBALBCYQAA\n",
              "rFAYAAArFMZV2My0++677yohIUExMTGKjY3VxIkTVVJS4olsW7duVXx8vOLj4xUbG6u0tLQrbo6s\n",
              "ynzfMMbo3nvvVUREhGeyvffee6pVq1bg/YuPj1dBQYEnsknSvn371KNHD7Vt21Zt27bVmjVrPJFt\n",
              "0aJFpd6z+vXra+DAgZ7IVlJSovHjxysmJkbt27dXUlJSpc1rZ5vvV7/6le644w5FR0dr+PDhunDh\n",
              "QqXkc8SgTElJSWbRokXGGGNWrlxp7rzzzivW2bVrl/n444+NMcYUFBSYu+++O7BNVWf76quvzIUL\n",
              "F4wxxhQXF5vk5GTz/PPPu57NNt83/vSnP5mUlBRTp04dz2TbtGmTiYuLq5Q8l7P9u952221m8+bN\n",
              "xhhjLl26ZPLy8jyR7dtiY2PNqlWrXE5ml23t2rUmISEh8G9i+vTp5qGHHnI9m22++fPnm6SkJFNU\n",
              "VGRKSkpMSkqKefbZZyslnxMURhk+++wzExYWZi5evGiMMaakpMQ0bNjQHD58+Du3GzNmjJk8ebLn\n",
              "shUUFJhevXqZF154wdVsTvNlZWWZbt26mezs7EopDNtsVVEYttkWLFhgHnnkEU9mu9wHH3xgGjRo\n",
              "EPgPuqqzvfrqqyYuLs74/X5TUlJifv3rX5tx48a5ms1JvjFjxpjf//73geerV6827dq1cz2fU1yS\n",
              "KsN3zbR7Nbm5uVq1apX69evnmWw5OTmKi4tT/fr1VadOHY0ePdrVbE7yXbx4USNGjFBGRkalTbzm\n",
              "5L37+OOP1bFjR911111KT0/3TLb9+/crJCRE/fr1U3x8vB5//HGdOnXKE9kut3DhQj322GO66aab\n",
              "PJHtgQceUI8ePdSoUSNFRUVp48aNmjZtmqvZnOTr1KmTXnvtNfn9fl28eFErVqxQTk6O6/mcojAq\n",
              "gN/v1wMPPKCJEyfqzjvvrOo4Abfeeqv27Nmj3NxcFRUVVcq1bltTp07VwIED1bZt26qOcoWOHTvq\n",
              "5MmT2rVrl9auXat58+ZpxYoVVR1LknTp0iW98847ysjI0O7du9WkSRONGjWqqmOV8tVXX2nZsmUa\n",
              "Pnx4VUcJ2LFjh7KysvTJJ5/of//7n370ox9p5MiRVR0rYOjQoerdu7e6d++u7t27q3Xr1oGS8ZSq\n",
              "PsXxIien4H6/33Tu3NlMnz7dc9kut3TpUtOvXz/P5Ovatatp1qyZad68uWnSpInx+XymefPmrl6P\n",
              "L+97N3PmTPPkk0+6lstJtj/+8Y/mscceCzzPysoyTZo08US2byxatMgkJia6mslptm9f8snKyjKN\n",
              "Gzf2TL5vW7p0qenatavr+ZziDKMMtjPtnjt3Tr1791bv3r31zDPPeCpbdna2Ll68KEm6cOGC1q5d\n",
              "q/bt23sm3+bNm3Xs2DHl5ORoy5YtCg8PV05Ojho0aFDl2T799NPAp93Onj2rzMxMdejQwbVcTrI9\n",
              "/PDD2r59u/x+vyRpw4YNiouL80S2byxcuLDSzi5ss7Vo0ULvvvtu4JNHmZmZuuOOOzyTr7CwUGfO\n",
              "nJEkff7555o9e7YmTpzoej7HqrqxvOrAgQMmMTHRtGrVynTq1Mns3bvXGGPM8OHDzbp164wxxsyY\n",
              "McPUqFHDxMXFBR4zZszwRLaMjAwTGxtr2rdvb2JiYszPf/5zU1BQ4Ho223yXO3r0aKV9Ssom25//\n",
              "/GcTExMTeO8mT55sSkpKPJHNGGP+/ve/m9jYWNOuXTvTu3dvc/z4cc9kO3DggAkNDTV+v9/1TE6y\n",
              "FRYWmpSUFBMdHW3atWtnevbsGfiEoxfy5ebmmujoaBMTE2Oio6PN3LlzKyWbU8wlBQCwwiUpAIAV\n",
              "CgMAYIXCAABYoTAAAFYoDACAFQoDAGCFwgAAWKEwAABWKAwAgBUKAwBghcIAAFj5P3MKHl34K9nr\n",
              "AAAAAElFTkSuQmCC\n",
              "\">\n",
              "        \n",
              "      </div>\n",
              "      <script></script>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-00441650-3e66-4861-b655-9e61a3e17276\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-00441650-3e66-4861-b655-9e61a3e17276\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}